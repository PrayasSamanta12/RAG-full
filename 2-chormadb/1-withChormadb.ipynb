{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d6815c",
   "metadata": {},
   "source": [
    "### How to work with Chormadb and store it in a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f5348",
   "metadata": {},
   "source": [
    "- Here we insert docs into db\n",
    "- Uses search operation on it\n",
    "- Get the context\n",
    "- Provide that context to the LLM with scores\n",
    "- Generate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a41006a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a0a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain + chormadb\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "#for chromadb\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from typing import List\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0561680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings_array = [\n",
    "    \"Python is renowned for its flexible data structures, which include lists, tuples, sets, and dictionaries. Lists are ordered collections that support dynamic resizing and a variety of methods for adding, removing, or searching elements. Tuples are similar to lists but immutable, ensuring that data remains unchanged once assigned and making them suitable for fixed data groupings. Dictionaries use key-value pairs allowing fast access, manipulation, and association of data by unique keys. Sets are collections of unordered, unique elements, great for removing duplicates and performing mathematical operations like unions and intersections. These data structures form the backbone for efficient algorithm development, making Python popular for data engineering, scientific computing, and rapid prototyping in diverse software projects.\"\n",
    "    ,\n",
    "    \"Docker containers have revolutionized application deployment by encapsulating all dependencies within lightweight, portable units. The container lifecycle runs through image creation, build, run, and destruction, allowing consistency across environments from a developer’s laptop to cloud production servers. Networking and persistent storage are handled through Docker’s bridge networks and mounted volumes, often configured in a YAML file for orchestration. Security features, resource limits, and automated health checks help maintain uptime and isolation. Command-line tools and APIs provide granular control, while platforms like Kubernetes extend management to large-scale clusters. Docker’s architecture enables microservices, CI/CD pipelines, and efficient scaling for modern software infrastructure.\"\n",
    "    ,\n",
    "    \"Vector databases have emerged as critical infrastructure for AI-driven applications by enabling the fast, approximate search of high-dimensional embeddings. Unlike traditional relational stores, vector DBs represent data points as mathematical vectors, supporting similarity queries, KNN search, and clustering. This approach underpins semantic retrieval in RAG systems, recommendation engines, and fraud detection. Technologies such as Pinecone, Weaviate, and ChromaDB offer APIs to store, update, and query embeddings generated by models like BERT or CLIP. They optimize for speed and scalability with techniques including approximate neighbors, distributed indexing, and GPU acceleration. Advanced filtering and metadata support enable hybrid retrieval for context-aware generative AI solutions.\"\n",
    "    ,\n",
    "    \"SQL query optimization is crucial for scalable database operations. Indexing frequently searched columns is an essential strategy, but too many indexes can degrade write performance. Queries should avoid ‘SELECT *’, minimize joins to essential tables, and use WHERE clauses that leverage indexed columns. Tools like EXPLAIN PLAN visualize execution steps, guiding developers to restructure queries for efficiency. Partitioning large tables can improve access speed while reducing locking contention. Regularly updating table statistics ensures the optimizer selects the best execution path. Avoiding correlated subqueries and using batch processing techniques can reduce resource consumption. These approaches together lead to faster, more reliable database systems.\"\n",
    "    ,\n",
    "    \"In Node.js, the event loop is a key mechanism that allows non-blocking I/O operations on a single thread. Each incoming request is delegated to the system kernel, freeing the JavaScript runtime to handle other events. Async callbacks are queued and executed when the kernel signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its event-driven model, combined with fast V8 execution, supports horizontal scaling and resource-efficient concurrency on modest hardware, making Node.js immensely popular for backend services.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b360b42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc created\n"
     ]
    }
   ],
   "source": [
    "# Storing the samlpes in a file\n",
    "import tempfile\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "for i,doc in enumerate(strings_array):\n",
    "    file_path = os.path.join(temp_dir, f\"doc_{i}.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(doc)\n",
    "\n",
    "print(\"Doc created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Document loading\n",
    "from langchain_community.document_loaders import DirectoryLoader,TextLoader\n",
    "load = DirectoryLoader(\n",
    "    temp_dir,\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "documents=load.load()\n",
    "for i,doc in enumerate(documents):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e640cdc",
   "metadata": {},
   "source": [
    "### Text Splitting from docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28eaa9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 15 of 5\n",
      "Content: page_content='Python is renowned for its flexible data structures, which include lists, tuples, sets, and dictionaries. Lists are ordered collections that support dynamic resizing and a variety of methods for adding, removing, or searching elements. Tuples are similar to lists but immutable, ensuring that data' metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmpb9s9tz0u\\\\doc_0.txt'}\n",
      "Metadata: {'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmpb9s9tz0u\\\\doc_0.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Text Splitting\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    separators=[\" \"]\n",
    ")\n",
    "chunks=splitter.split_documents(documents)\n",
    "print(f\"Total chunks: {len(chunks)} of {len(documents)}\")\n",
    "print(f\"Content: {chunks[0]}\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551f490",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3856e937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.043933555483818054,\n",
       " 0.05893440172076225,\n",
       " 0.04817839711904526,\n",
       " 0.07754813879728317,\n",
       " 0.02674437128007412,\n",
       " -0.03762954846024513,\n",
       " -0.0026051148306578398,\n",
       " -0.05994303897023201,\n",
       " -0.0024960089940577745,\n",
       " 0.02207283116877079,\n",
       " 0.048025909811258316,\n",
       " 0.055755313485860825,\n",
       " -0.03894546255469322,\n",
       " -0.026616770774126053,\n",
       " 0.0076933917589485645,\n",
       " -0.026237700134515762,\n",
       " -0.03641606494784355,\n",
       " -0.03781614825129509,\n",
       " 0.07407816499471664,\n",
       " -0.04950505867600441,\n",
       " -0.05852171406149864,\n",
       " -0.06361967325210571,\n",
       " 0.032435014843940735,\n",
       " 0.022008540108799934,\n",
       " -0.07106371223926544,\n",
       " -0.03315779194235802,\n",
       " -0.06941041350364685,\n",
       " -0.05003739148378372,\n",
       " 0.07462679594755173,\n",
       " -0.11113381385803223,\n",
       " -0.01230629812926054,\n",
       " 0.03774565830826759,\n",
       " -0.02803134173154831,\n",
       " 0.014535323716700077,\n",
       " -0.031558554619550705,\n",
       " -0.08058364689350128,\n",
       " 0.05835256725549698,\n",
       " 0.002590067917481065,\n",
       " 0.0392802357673645,\n",
       " 0.025769580155611038,\n",
       " 0.049850545823574066,\n",
       " -0.0017562442226335406,\n",
       " -0.04552978649735451,\n",
       " 0.029260773211717606,\n",
       " -0.10201726108789444,\n",
       " 0.05222873389720917,\n",
       " -0.07908995449542999,\n",
       " -0.010285736061632633,\n",
       " 0.009202471934258938,\n",
       " 0.013073218055069447,\n",
       " -0.04047776013612747,\n",
       " -0.02779252640902996,\n",
       " 0.01246674731373787,\n",
       " 0.06728330999612808,\n",
       " 0.06812482327222824,\n",
       " -0.007571159861981869,\n",
       " -0.006099394988268614,\n",
       " -0.042377717792987823,\n",
       " 0.05178157985210419,\n",
       " -0.015670742839574814,\n",
       " 0.009563641622662544,\n",
       " 0.04123904928565025,\n",
       " 0.021495940163731575,\n",
       " 0.010429336689412594,\n",
       " 0.027334952726960182,\n",
       " 0.01870625466108322,\n",
       " -0.026960738003253937,\n",
       " -0.07005421072244644,\n",
       " -0.10470050573348999,\n",
       " -0.0018987420480698347,\n",
       " 0.017701663076877594,\n",
       " -0.057472579181194305,\n",
       " -0.014422297477722168,\n",
       " 0.00047048315173015,\n",
       " 0.0023323262576013803,\n",
       " -0.025192037224769592,\n",
       " 0.04930044710636139,\n",
       " -0.050961028784513474,\n",
       " 0.06319833546876907,\n",
       " 0.014916478656232357,\n",
       " -0.027076713740825653,\n",
       " -0.04528754949569702,\n",
       " -0.04905946925282478,\n",
       " 0.03749404102563858,\n",
       " 0.03845794126391411,\n",
       " 0.001568990875966847,\n",
       " 0.03099225088953972,\n",
       " 0.020163079723715782,\n",
       " -0.012436363846063614,\n",
       " -0.030672011896967888,\n",
       " -0.0278819240629673,\n",
       " -0.06891823559999466,\n",
       " -0.051367755979299545,\n",
       " 0.021479541435837746,\n",
       " 0.01157472562044859,\n",
       " 0.001254115253686905,\n",
       " 0.018876584246754646,\n",
       " -0.04423188045620918,\n",
       " -0.044981759041547775,\n",
       " -0.003418722189962864,\n",
       " 0.013113114982843399,\n",
       " 0.020009934902191162,\n",
       " 0.12109974771738052,\n",
       " 0.02310747653245926,\n",
       " -0.02201596088707447,\n",
       " -0.03288467600941658,\n",
       " -0.0031551308929920197,\n",
       " 0.00011783662193920463,\n",
       " 0.09914986044168472,\n",
       " 0.016523893922567368,\n",
       " -0.0046967146918177605,\n",
       " -0.014536636881530285,\n",
       " -0.0037108040414750576,\n",
       " 0.09651362150907516,\n",
       " 0.02859078347682953,\n",
       " 0.02134820632636547,\n",
       " -0.0717645063996315,\n",
       " -0.02411423996090889,\n",
       " -0.04409408941864967,\n",
       " -0.10734688490629196,\n",
       " 0.06799457222223282,\n",
       " 0.1304667741060257,\n",
       " -0.0797029510140419,\n",
       " 0.006795077119022608,\n",
       " -0.02375120110809803,\n",
       " -0.046163659542798996,\n",
       " -0.029965076595544815,\n",
       " -3.6941008327124435e-33,\n",
       " 0.07309703528881073,\n",
       " -0.022017158567905426,\n",
       " -0.08614647388458252,\n",
       " -0.07143795490264893,\n",
       " -0.06367422640323639,\n",
       " -0.07218635827302933,\n",
       " -0.005930447019636631,\n",
       " -0.02336413599550724,\n",
       " -0.028365792706608772,\n",
       " 0.04774349555373192,\n",
       " -0.08061760663986206,\n",
       " -0.001564762438647449,\n",
       " 0.013844374567270279,\n",
       " -0.028623618185520172,\n",
       " -0.03353862091898918,\n",
       " -0.11377756297588348,\n",
       " -0.00917635578662157,\n",
       " -0.010810134001076221,\n",
       " 0.032319575548172,\n",
       " 0.05883808061480522,\n",
       " 0.033420857042074203,\n",
       " 0.1079879105091095,\n",
       " -0.03727130591869354,\n",
       " -0.02967708557844162,\n",
       " 0.0517190545797348,\n",
       " -0.022533906623721123,\n",
       " -0.06960906088352203,\n",
       " -0.021447530016303062,\n",
       " -0.02334107831120491,\n",
       " 0.04821997135877609,\n",
       " -0.03587663546204567,\n",
       " -0.04689909145236015,\n",
       " -0.03978736326098442,\n",
       " 0.11081323772668839,\n",
       " -0.01430070586502552,\n",
       " -0.11846451461315155,\n",
       " 0.058291513472795486,\n",
       " -0.06258892267942429,\n",
       " -0.02940407767891884,\n",
       " 0.060323845595121384,\n",
       " -0.0024441315326839685,\n",
       " 0.01601160131394863,\n",
       " 0.026723401620984077,\n",
       " 0.024953022599220276,\n",
       " -0.06493189930915833,\n",
       " -0.010680188424885273,\n",
       " 0.028146496042609215,\n",
       " 0.010356348007917404,\n",
       " -0.0006636060425080359,\n",
       " 0.01981864497065544,\n",
       " -0.03042880818247795,\n",
       " 0.006284233182668686,\n",
       " 0.05152680724859238,\n",
       " -0.04753753915429115,\n",
       " -0.06444213539361954,\n",
       " 0.09550314396619797,\n",
       " 0.07558581233024597,\n",
       " -0.028157468885183334,\n",
       " -0.03499656543135643,\n",
       " 0.10181640088558197,\n",
       " 0.019873222336173058,\n",
       " -0.036803700029850006,\n",
       " 0.002935214899480343,\n",
       " -0.05007451772689819,\n",
       " 0.1509321630001068,\n",
       " -0.061607927083969116,\n",
       " -0.0858813226222992,\n",
       " 0.007139905821532011,\n",
       " -0.01330658234655857,\n",
       " 0.07804052531719208,\n",
       " 0.0175250805914402,\n",
       " 0.04212793707847595,\n",
       " 0.0357939712703228,\n",
       " -0.13295048475265503,\n",
       " 0.03569706901907921,\n",
       " -0.02031167596578598,\n",
       " 0.012490971013903618,\n",
       " -0.0380355566740036,\n",
       " 0.04915430396795273,\n",
       " -0.01565408892929554,\n",
       " 0.12141826003789902,\n",
       " -0.08086445182561874,\n",
       " -0.046878181397914886,\n",
       " 0.04108429700136185,\n",
       " -0.018431775271892548,\n",
       " 0.06696905940771103,\n",
       " 0.004335932899266481,\n",
       " 0.022731535136699677,\n",
       " -0.013642888516187668,\n",
       " -0.045323844999074936,\n",
       " -0.039282944053411484,\n",
       " -0.006298864725977182,\n",
       " 0.05296100676059723,\n",
       " -0.03690645843744278,\n",
       " 0.07116773724555969,\n",
       " 2.3334323261730595e-33,\n",
       " 0.105231374502182,\n",
       " -0.04818745329976082,\n",
       " 0.06959188729524612,\n",
       " 0.06569761782884598,\n",
       " -0.04651489481329918,\n",
       " 0.05144922807812691,\n",
       " -0.012447536922991276,\n",
       " 0.03208721801638603,\n",
       " -0.09233567118644714,\n",
       " 0.050093282014131546,\n",
       " -0.03288758918642998,\n",
       " 0.013913878239691257,\n",
       " -0.0008702629711478949,\n",
       " -0.004909062292426825,\n",
       " 0.10394640266895294,\n",
       " 0.00032157512032426894,\n",
       " 0.052811022847890854,\n",
       " -0.011799043975770473,\n",
       " 0.023156559094786644,\n",
       " 0.013176850974559784,\n",
       " -0.052596259862184525,\n",
       " 0.0326702781021595,\n",
       " 0.00030868290923535824,\n",
       " 0.06411287933588028,\n",
       " 0.03885011374950409,\n",
       " 0.05880085006356239,\n",
       " 0.08297933638095856,\n",
       " -0.018814953044056892,\n",
       " -0.022637730464339256,\n",
       " -0.10047364979982376,\n",
       " -0.038375284522771835,\n",
       " -0.058808136731386185,\n",
       " 0.0018241782672703266,\n",
       " -0.04269954562187195,\n",
       " 0.025019558146595955,\n",
       " 0.06400599330663681,\n",
       " -0.03774828463792801,\n",
       " -0.006839035078883171,\n",
       " -0.0025460815522819757,\n",
       " -0.09760429710149765,\n",
       " 0.018847594037652016,\n",
       " -0.0008832094608806074,\n",
       " 0.017361199483275414,\n",
       " 0.07107910513877869,\n",
       " 0.03303929418325424,\n",
       " 0.006934212055057287,\n",
       " -0.05605234205722809,\n",
       " 0.05146351084113121,\n",
       " -0.04295423626899719,\n",
       " 0.046007778495550156,\n",
       " -0.008788290433585644,\n",
       " 0.031728923320770264,\n",
       " 0.04939655587077141,\n",
       " 0.02951902151107788,\n",
       " -0.05051926150918007,\n",
       " -0.05431870371103287,\n",
       " 0.0001499512291047722,\n",
       " -0.027661476284265518,\n",
       " 0.034687891602516174,\n",
       " -0.02108898013830185,\n",
       " 0.013805982656776905,\n",
       " 0.029988665133714676,\n",
       " 0.013974461704492569,\n",
       " -0.0042647188529372215,\n",
       " -0.015033715404570103,\n",
       " -0.08760954439640045,\n",
       " -0.06850539892911911,\n",
       " -0.042814210057258606,\n",
       " 0.07769452780485153,\n",
       " -0.07102853804826736,\n",
       " -0.007376902736723423,\n",
       " 0.021372701972723007,\n",
       " 0.01355623360723257,\n",
       " -0.07904644310474396,\n",
       " 0.005476667080074549,\n",
       " 0.08306633681058884,\n",
       " 0.11414802819490433,\n",
       " 0.0018076023552566767,\n",
       " 0.08754914999008179,\n",
       " -0.04160451143980026,\n",
       " 0.015541651286184788,\n",
       " -0.010120656341314316,\n",
       " -0.007324368692934513,\n",
       " 0.010796631686389446,\n",
       " -0.06628164649009705,\n",
       " 0.039841361343860626,\n",
       " -0.1167115643620491,\n",
       " 0.06429939717054367,\n",
       " 0.040291961282491684,\n",
       " -0.06547418981790543,\n",
       " 0.01950530707836151,\n",
       " 0.08099953085184097,\n",
       " 0.05364638566970825,\n",
       " 0.07679696381092072,\n",
       " -0.013485214672982693,\n",
       " -1.769190305367374e-08,\n",
       " -0.04439355432987213,\n",
       " 0.009206428192555904,\n",
       " -0.08795901387929916,\n",
       " 0.0426921583712101,\n",
       " 0.07313650846481323,\n",
       " 0.016842715442180634,\n",
       " -0.04032626375555992,\n",
       " 0.018513141199946404,\n",
       " 0.08441726863384247,\n",
       " -0.037447698414325714,\n",
       " 0.030299631878733635,\n",
       " 0.02906418964266777,\n",
       " 0.06368789076805115,\n",
       " 0.02897503972053528,\n",
       " -0.014726998284459114,\n",
       " 0.017754288390278816,\n",
       " -0.033689551055431366,\n",
       " 0.017316123470664024,\n",
       " 0.033787526190280914,\n",
       " 0.17682604491710663,\n",
       " -0.01755334623157978,\n",
       " -0.060307826846838,\n",
       " -0.014339499175548553,\n",
       " -0.02385362610220909,\n",
       " -0.044553130865097046,\n",
       " -0.028985051438212395,\n",
       " -0.0896776095032692,\n",
       " -0.0017593814991414547,\n",
       " -0.026148563250899315,\n",
       " 0.005939967464655638,\n",
       " -0.05183551833033562,\n",
       " 0.08572796732187271,\n",
       " -0.08183988928794861,\n",
       " 0.008354436606168747,\n",
       " 0.04007899761199951,\n",
       " 0.04177643358707428,\n",
       " 0.1045735701918602,\n",
       " -0.002865632064640522,\n",
       " 0.019669104367494583,\n",
       " 0.005810464732348919,\n",
       " 0.013325355015695095,\n",
       " 0.045100126415491104,\n",
       " -0.021758832037448883,\n",
       " -0.013949315994977951,\n",
       " -0.06869927048683167,\n",
       " -0.002941165817901492,\n",
       " -0.031076528131961823,\n",
       " -0.10585445165634155,\n",
       " 0.06916242837905884,\n",
       " -0.04241151735186577,\n",
       " -0.04676824063062668,\n",
       " -0.03647506237030029,\n",
       " 0.04504002630710602,\n",
       " 0.0609816312789917,\n",
       " -0.0656561627984047,\n",
       " -0.00545640429481864,\n",
       " -0.01862267404794693,\n",
       " -0.06314847618341446,\n",
       " -0.03874368965625763,\n",
       " 0.03467337042093277,\n",
       " 0.0555458590388298,\n",
       " 0.05216279253363609,\n",
       " 0.05610649287700653,\n",
       " 0.10206393152475357]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "embeddings=HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    ")\n",
    "embeddings.embed_query(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1181460e",
   "metadata": {},
   "source": [
    "#### Storing into ChromaDB using HuggingFace Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adfbb696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ps19j\\AppData\\Local\\Temp\\ipykernel_3192\\3120618224.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  VECTOR_STORE = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created at ./chromaDB\n",
      "Number of vectors: 35\n"
     ]
    }
   ],
   "source": [
    "# Directory of ChromaDB\n",
    "persistant_directory = \"./chromaDB\"\n",
    "\n",
    "VECTOR_STORE = Chroma(\n",
    "    persist_directory=persistant_directory,\n",
    "    embedding_function=HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    ),\n",
    "    collection_name=\"Rag_collection\"\n",
    ")\n",
    "VECTOR_STORE.add_documents(chunks)\n",
    "print(f\"Vector store created at {persistant_directory}\")\n",
    "print(f\"Number of vectors: {VECTOR_STORE._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c5738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmppjdesij8\\\\doc_4.txt'}, page_content='signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its'), Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmpb9s9tz0u\\\\doc_4.txt'}, page_content='signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its'), Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmppjdesij8\\\\doc_4.txt'}, page_content='In Node.js, the event loop is a key mechanism that allows non-blocking I/O operations on a single thread. Each incoming request is delegated to the system kernel, freeing the JavaScript runtime to handle other events. Async callbacks are queued and executed when the kernel signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its event-driven model, combined with fast V8 execution, supports horizontal scaling and resource-efficient concurrency on modest hardware, making Node.js immensely popular for backend services.')]\n",
      "[(Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmppjdesij8\\\\doc_4.txt'}, page_content='signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its'), 0.8995416164398193), (Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmpb9s9tz0u\\\\doc_4.txt'}, page_content='signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its'), 0.8995416164398193), (Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmppjdesij8\\\\doc_4.txt'}, page_content='In Node.js, the event loop is a key mechanism that allows non-blocking I/O operations on a single thread. Each incoming request is delegated to the system kernel, freeing the JavaScript runtime to handle other events. Async callbacks are queued and executed when the kernel signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its event-driven model, combined with fast V8 execution, supports horizontal scaling and resource-efficient concurrency on modest hardware, making Node.js immensely popular for backend services.'), 0.9453365802764893)]\n"
     ]
    }
   ],
   "source": [
    "query=\"How to work with Node js?\"\n",
    "similar_chunks=VECTOR_STORE.similarity_search(query, k=3)\n",
    "print(similar_chunks)\n",
    "similar_chunks_with_score=VECTOR_STORE.similarity_search_with_score(query,k=3)\n",
    "print(similar_chunks_with_score)# it prints the similar chunks with their scores or ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "load_dotenv()\n",
    "OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df191e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using free LLM to know the working of Agentic AI\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# First API call with reasoning\n",
    "response = client.chat.completions.create(\n",
    "  model=\"x-ai/grok-4.1-fast\",\n",
    "  messages=[\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you explain me about Agentic AI?\"\n",
    "          }\n",
    "        ],\n",
    "  extra_body={\"reasoning\": {\"enabled\": True}}\n",
    ")\n",
    "response=response.choices[0].message.content\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16f98a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"**Paris** is the capital of France. It's located in the north-central part of the country along the Seine River and is famous for landmarks like the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 122, 'prompt_tokens': 163, 'total_tokens': 285, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 81, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'x-ai/grok-4.1-fast', 'system_fingerprint': None, 'id': 'gen-1764070297-udtHQqyafZRhDxw6Kz2M', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--0fb0e2e8-30f6-4e79-9f51-5b8b816e3571-0' usage_metadata={'input_tokens': 163, 'output_tokens': 122, 'total_tokens': 285, 'input_token_details': {}, 'output_token_details': {'reasoning': 81}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(\n",
    "    model=\"x-ai/grok-4.1-fast\",\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "response=llm.invoke(\"What is the capital of France?\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64220e",
   "metadata": {},
   "source": [
    "# Using RAG Chains with Chormadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02da99a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002563D8CFCB0>, search_kwargs={'k': 4})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With out using Langchain Expression Language\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "retriever=VECTOR_STORE.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4},\n",
    ")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71cb1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom prompts\n",
    "system_prompt=\"\"\"\n",
    "Use the following chunks as reference for the answer of the question \n",
    "Provide a answer of nearly 200-300 words and use the chunks to gather the answer also use your own knowledge and experience\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ca452b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nUse the following chunks as reference for the answer of the question \\nProvide a answer of nearly 200-300 words and use the chunks to gather the answer also use your own knowledge and experience\\n\\nContext:\\n{context}\\n\\nAnswer:\\n\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002563FC0B890>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002563FC3AF90>, root_client=<openai.OpenAI object at 0x000002563FC0B610>, root_async_client=<openai.AsyncOpenAI object at 0x000002563FC3ACF0>, model_name='x-ai/grok-4.1-fast', model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://openrouter.ai/api/v1')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting in the chain using create_stuff_document_chain-> It will help to combine all the chunks and provide that to the prompt and then that will be passed to the LLM\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381136c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The total chain is something like this:\\n\n",
    "Input → Format Documents → Create Prompt → LLM Call → Parse Output<br>\n",
    "**Now what internally happens is that RunnableBinding wraps the whole chain and in that at first it accumulates\n",
    "the context using format_docs and then it provide that to the ChatPromptTemplate and then it passed it to the ChatOpenAI , the OpenAI provides a response which later showed using StrOutputParser()**\n",
    "\n",
    "- The format_docs() function is responsible for formatting the documents into a single string.\n",
    "```\n",
    "def format_docs(chunks):\n",
    "return \"\\n\\n\".join(chunks.page_content for doc in docs)\n",
    "```\n",
    "\n",
    "- The ChatPromptTemplate() function is responsible for creating the prompt for the LLM.\n",
    "```\n",
    "ChatPromptTemplate(\n",
    "    input_variables=['context', 'question'],\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                template='''Use the following chunks as reference for the answer of the question \n",
    "Provide a answer of nearly 200-300 words and use the chunks to gather the answer also use your own knowledge and experience\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\n",
    "'''\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(template='{question}')\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "- The ChatOpenAI() function is responsible for calling the LLM.\n",
    "```ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=1000)```\n",
    "\n",
    "- The StrOutputParser() function is responsible for parsing the output of the LLM.\n",
    "```StrOutputParser(output_key='text')```\n",
    "\n",
    "- The runnable_binding is responsible for binding the above functions together.\n",
    "```runnable_binding = RunnableBinding(format_docs, ChatPromptTemplate, ChatOpenAI, StrOutputParser)```\n",
    "\n",
    "- The runnable is something like this:\\n\n",
    "```runnable = runnable_binding.bind(context=context, question=question)```\n",
    "\n",
    "- The runnable is something like this:\\n\n",
    "```runnable.run()```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b343af06",
   "metadata": {},
   "source": [
    "#### Final Rag with both Chunks and Chormadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05efadbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002563D8CFCB0>, search_kwargs={'k': 4}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nUse the following chunks as reference for the answer of the question \\nProvide a answer of nearly 200-300 words and use the chunks to gather the answer also use your own knowledge and experience\\n\\nContext:\\n{context}\\n\\nAnswer:\\n\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002563FC0B890>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002563FC3AF90>, root_client=<openai.OpenAI object at 0x000002563FC0B610>, root_async_client=<openai.AsyncOpenAI object at 0x000002563FC3ACF0>, model_name='x-ai/grok-4.1-fast', model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://openrouter.ai/api/v1')\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain=create_retrieval_chain(retriever,document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1985c0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'I want to know about vector database as a beginner', 'context': [Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmppjdesij8\\\\doc_2.txt'}, page_content='Vector databases have emerged as critical infrastructure for AI-driven applications by enabling the fast, approximate search of high-dimensional embeddings. Unlike traditional relational stores, vector DBs represent data points as mathematical vectors, supporting similarity queries, KNN search, and'), Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmpb9s9tz0u\\\\doc_2.txt'}, page_content='Vector databases have emerged as critical infrastructure for AI-driven applications by enabling the fast, approximate search of high-dimensional embeddings. Unlike traditional relational stores, vector DBs represent data points as mathematical vectors, supporting similarity queries, KNN search, and'), Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmppjdesij8\\\\doc_2.txt'}, page_content='Vector databases have emerged as critical infrastructure for AI-driven applications by enabling the fast, approximate search of high-dimensional embeddings. Unlike traditional relational stores, vector DBs represent data points as mathematical vectors, supporting similarity queries, KNN search, and clustering. This approach underpins semantic retrieval in RAG systems, recommendation engines, and fraud detection. Technologies such as Pinecone, Weaviate, and ChromaDB offer APIs to store, update, and query embeddings generated by models like BERT or CLIP. They optimize for speed and scalability with techniques including approximate neighbors, distributed indexing, and GPU acceleration. Advanced filtering and metadata support enable hybrid retrieval for context-aware generative AI solutions.'), Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmppjdesij8\\\\doc_3.txt'}, page_content='selects the best execution path. Avoiding correlated subqueries and using batch processing techniques can reduce resource consumption. These approaches together lead to faster, more reliable database systems.')], 'answer': '### What is a Vector Database? A Beginner\\'s Guide\\n\\nImagine you\\'re trying to find a book in a massive library not by its title or author, but by how similar its content feels to what you\\'re thinking—like \"stories about brave adventurers in magical worlds.\" Traditional databases (like SQL ones) excel at exact matches: \"Give me the book titled *Harry Potter*.\" But vector databases are built for **similarity searches**, treating data as points in a high-dimensional space.\\n\\nAt their core, vector databases store **embeddings**—numerical vectors (arrays of numbers) that represent things like text, images, or audio. For example, AI models like BERT or OpenAI\\'s embeddings turn a sentence like \"I love cats\" into a vector [0.2, -0.5, 1.3, ...] capturing its meaning. These vectors live in hundreds or thousands of dimensions, where similar items cluster close together.\\n\\nUnlike relational databases with rows and tables, vector DBs use math like **cosine similarity** or **Euclidean distance** to find the **nearest neighbors (KNN)** quickly. They rely on **approximate nearest neighbors (ANN)** algorithms (e.g., HNSW or IVF) for speed on massive datasets—think billions of vectors—without scanning everything.\\n\\n**Why do they matter?** They\\'re the backbone of modern AI apps:\\n- **RAG (Retrieval-Augmented Generation)**: ChatGPT-like systems pull relevant docs for accurate answers.\\n- **Recommendations**: Netflix suggests shows based on your tastes.\\n- **Search & Fraud Detection**: Find similar images or spot anomalies.\\n\\nPopular open-source/free options: **ChromaDB** (super simple for local use), **Weaviate** (GraphQL-friendly with hybrid search), **Pinecone** (managed cloud service). Getting started? Install Chroma via pip, embed text with Hugging Face, and query in Python—it\\'s beginner-friendly!\\n\\nVector DBs scale with GPU acceleration, distributed indexes, and metadata filtering for real-world apps. They\\'re exploding in AI because they make \"fuzzy\" searches lightning-fast. Dive in with a tutorial; you\\'ll see why they\\'re essential infrastructure!\\n\\n*(Word count: 278)*'}\n"
     ]
    }
   ],
   "source": [
    "# Final chain\n",
    "# Retriver will retrive based on query from the db -> retreived dataw will passed as context\n",
    "# -> context will be passed to the LLM\n",
    "# -> LLM will generate a response based on the custom prompt\n",
    "\n",
    "last_response=rag_chain.invoke({\"input\":\"I want to know about vector database as a beginner\"})\n",
    "print(last_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f86d3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### What is a Vector Database? A Beginner\\'s Guide\\n\\nImagine you\\'re trying to find a book in a massive library not by its title or author, but by how similar its content feels to what you\\'re thinking—like \"stories about brave adventurers in magical worlds.\" Traditional databases (like SQL ones) excel at exact matches: \"Give me the book titled *Harry Potter*.\" But vector databases are built for **similarity searches**, treating data as points in a high-dimensional space.\\n\\nAt their core, vector databases store **embeddings**—numerical vectors (arrays of numbers) that represent things like text, images, or audio. For example, AI models like BERT or OpenAI\\'s embeddings turn a sentence like \"I love cats\" into a vector [0.2, -0.5, 1.3, ...] capturing its meaning. These vectors live in hundreds or thousands of dimensions, where similar items cluster close together.\\n\\nUnlike relational databases with rows and tables, vector DBs use math like **cosine similarity** or **Euclidean distance** to find the **nearest neighbors (KNN)** quickly. They rely on **approximate nearest neighbors (ANN)** algorithms (e.g., HNSW or IVF) for speed on massive datasets—think billions of vectors—without scanning everything.\\n\\n**Why do they matter?** They\\'re the backbone of modern AI apps:\\n- **RAG (Retrieval-Augmented Generation)**: ChatGPT-like systems pull relevant docs for accurate answers.\\n- **Recommendations**: Netflix suggests shows based on your tastes.\\n- **Search & Fraud Detection**: Find similar images or spot anomalies.\\n\\nPopular open-source/free options: **ChromaDB** (super simple for local use), **Weaviate** (GraphQL-friendly with hybrid search), **Pinecone** (managed cloud service). Getting started? Install Chroma via pip, embed text with Hugging Face, and query in Python—it\\'s beginner-friendly!\\n\\nVector DBs scale with GPU acceleration, distributed indexes, and metadata filtering for real-world apps. They\\'re exploding in AI because they make \"fuzzy\" searches lightning-fast. Dive in with a tutorial; you\\'ll see why they\\'re essential infrastructure!\\n\\n*(Word count: 278)*'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b31a9",
   "metadata": {},
   "source": [
    "'### What is a Vector Database? A Beginner\\'s Guide\\n\\nImagine you\\'re trying to find a book in a massive library not by its title or author, but by how similar its content feels to what you\\'re thinking—like \"stories about brave adventurers in magical worlds.\" Traditional databases (like SQL ones) excel at exact matches: \"Give me the book titled *Harry Potter*.\" But vector databases are built for **similarity searches**, treating data as points in a high-dimensional space.\\n\\nAt their core, vector databases store **embeddings**—numerical vectors (arrays of numbers) that represent things like text, images, or audio. For example, AI models like BERT or OpenAI\\'s embeddings turn a sentence like \"I love cats\" into a vector [0.2, -0.5, 1.3, ...] capturing its meaning. These vectors live in hundreds or thousands of dimensions, where similar items cluster close together.\\n\\nUnlike relational databases with rows and tables, vector DBs use math like **cosine similarity** or **Euclidean distance** to find the **nearest neighbors (KNN)** quickly. They rely on **approximate nearest neighbors (ANN)** algorithms (e.g., HNSW or IVF) for speed on massive datasets—think billions of vectors—without scanning everything.\\n\\n**Why do they matter?** They\\'re the backbone of modern AI apps:\\n- **RAG (Retrieval-Augmented Generation)**: ChatGPT-like systems pull relevant docs for accurate answers.\\n- **Recommendations**: Netflix suggests shows based on your tastes.\\n- **Search & Fraud Detection**: Find similar images or spot anomalies.\\n\\nPopular open-source/free options: **ChromaDB** (super simple for local use), **Weaviate** (GraphQL-friendly with hybrid search), **Pinecone** (managed cloud service). Getting started? Install Chroma via pip, embed text with Hugging Face, and query in Python—it\\'s beginner-friendly!\\n\\nVector DBs scale with GPU acceleration, distributed indexes, and metadata filtering for real-world apps. They\\'re exploding in AI because they make \"fuzzy\" searches lightning-fast. Dive in with a tutorial; you\\'ll see why they\\'re essential infrastructure!\\n\\n*(Word count: 278)*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9961145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-proj (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

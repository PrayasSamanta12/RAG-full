{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d6815c",
   "metadata": {},
   "source": [
    "### How to work with Chormadb and store it in a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f5348",
   "metadata": {},
   "source": [
    "- Here we insert docs into db\n",
    "- Uses search operation on it\n",
    "- Get the context\n",
    "- Provide that context to the LLM with scores\n",
    "- Generate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a41006a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a0a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain + chormadb\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "#for chromadb\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from typing import List\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0561680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings_array = [\n",
    "    \"Python is renowned for its flexible data structures, which include lists, tuples, sets, and dictionaries. Lists are ordered collections that support dynamic resizing and a variety of methods for adding, removing, or searching elements. Tuples are similar to lists but immutable, ensuring that data remains unchanged once assigned and making them suitable for fixed data groupings. Dictionaries use key-value pairs allowing fast access, manipulation, and association of data by unique keys. Sets are collections of unordered, unique elements, great for removing duplicates and performing mathematical operations like unions and intersections. These data structures form the backbone for efficient algorithm development, making Python popular for data engineering, scientific computing, and rapid prototyping in diverse software projects.\"\n",
    "    ,\n",
    "    \"Docker containers have revolutionized application deployment by encapsulating all dependencies within lightweight, portable units. The container lifecycle runs through image creation, build, run, and destruction, allowing consistency across environments from a developerâ€™s laptop to cloud production servers. Networking and persistent storage are handled through Dockerâ€™s bridge networks and mounted volumes, often configured in a YAML file for orchestration. Security features, resource limits, and automated health checks help maintain uptime and isolation. Command-line tools and APIs provide granular control, while platforms like Kubernetes extend management to large-scale clusters. Dockerâ€™s architecture enables microservices, CI/CD pipelines, and efficient scaling for modern software infrastructure.\"\n",
    "    ,\n",
    "    \"Vector databases have emerged as critical infrastructure for AI-driven applications by enabling the fast, approximate search of high-dimensional embeddings. Unlike traditional relational stores, vector DBs represent data points as mathematical vectors, supporting similarity queries, KNN search, and clustering. This approach underpins semantic retrieval in RAG systems, recommendation engines, and fraud detection. Technologies such as Pinecone, Weaviate, and ChromaDB offer APIs to store, update, and query embeddings generated by models like BERT or CLIP. They optimize for speed and scalability with techniques including approximate neighbors, distributed indexing, and GPU acceleration. Advanced filtering and metadata support enable hybrid retrieval for context-aware generative AI solutions.\"\n",
    "    ,\n",
    "    \"SQL query optimization is crucial for scalable database operations. Indexing frequently searched columns is an essential strategy, but too many indexes can degrade write performance. Queries should avoid â€˜SELECT *â€™, minimize joins to essential tables, and use WHERE clauses that leverage indexed columns. Tools like EXPLAIN PLAN visualize execution steps, guiding developers to restructure queries for efficiency. Partitioning large tables can improve access speed while reducing locking contention. Regularly updating table statistics ensures the optimizer selects the best execution path. Avoiding correlated subqueries and using batch processing techniques can reduce resource consumption. These approaches together lead to faster, more reliable database systems.\"\n",
    "    ,\n",
    "    \"In Node.js, the event loop is a key mechanism that allows non-blocking I/O operations on a single thread. Each incoming request is delegated to the system kernel, freeing the JavaScript runtime to handle other events. Async callbacks are queued and executed when the kernel signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its event-driven model, combined with fast V8 execution, supports horizontal scaling and resource-efficient concurrency on modest hardware, making Node.js immensely popular for backend services.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b360b42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc created\n"
     ]
    }
   ],
   "source": [
    "# Storing the samlpes in a file\n",
    "import tempfile\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "for i,doc in enumerate(strings_array):\n",
    "    file_path = os.path.join(temp_dir, f\"doc_{i}.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(doc)\n",
    "\n",
    "print(\"Doc created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33fcc64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0:\n",
      "Python is renowned for its flexible data structures, which include lists, tuples, sets, and dictionaries. Lists are ordered collections that support dynamic resizing and a variety of methods for adding, removing, or searching elements. Tuples are similar to lists but immutable, ensuring that data remains unchanged once assigned and making them suitable for fixed data groupings. Dictionaries use key-value pairs allowing fast access, manipulation, and association of data by unique keys. Sets are collections of unordered, unique elements, great for removing duplicates and performing mathematical operations like unions and intersections. These data structures form the backbone for efficient algorithm development, making Python popular for data engineering, scientific computing, and rapid prototyping in diverse software projects.\n",
      "Metadata: {'source': 'C:\\\\Users\\\\samantap\\\\AppData\\\\Local\\\\Temp\\\\tmpkx50hwz6\\\\doc_0.txt'}\n",
      "Document 1:\n",
      "Docker containers have revolutionized application deployment by encapsulating all dependencies within lightweight, portable units. The container lifecycle runs through image creation, build, run, and destruction, allowing consistency across environments from a developerâ€™s laptop to cloud production servers. Networking and persistent storage are handled through Dockerâ€™s bridge networks and mounted volumes, often configured in a YAML file for orchestration. Security features, resource limits, and automated health checks help maintain uptime and isolation. Command-line tools and APIs provide granular control, while platforms like Kubernetes extend management to large-scale clusters. Dockerâ€™s architecture enables microservices, CI/CD pipelines, and efficient scaling for modern software infrastructure.\n",
      "Metadata: {'source': 'C:\\\\Users\\\\samantap\\\\AppData\\\\Local\\\\Temp\\\\tmpkx50hwz6\\\\doc_1.txt'}\n",
      "Document 2:\n",
      "Vector databases have emerged as critical infrastructure for AI-driven applications by enabling the fast, approximate search of high-dimensional embeddings. Unlike traditional relational stores, vector DBs represent data points as mathematical vectors, supporting similarity queries, KNN search, and clustering. This approach underpins semantic retrieval in RAG systems, recommendation engines, and fraud detection. Technologies such as Pinecone, Weaviate, and ChromaDB offer APIs to store, update, and query embeddings generated by models like BERT or CLIP. They optimize for speed and scalability with techniques including approximate neighbors, distributed indexing, and GPU acceleration. Advanced filtering and metadata support enable hybrid retrieval for context-aware generative AI solutions.\n",
      "Metadata: {'source': 'C:\\\\Users\\\\samantap\\\\AppData\\\\Local\\\\Temp\\\\tmpkx50hwz6\\\\doc_2.txt'}\n",
      "Document 3:\n",
      "SQL query optimization is crucial for scalable database operations. Indexing frequently searched columns is an essential strategy, but too many indexes can degrade write performance. Queries should avoid â€˜SELECT *â€™, minimize joins to essential tables, and use WHERE clauses that leverage indexed columns. Tools like EXPLAIN PLAN visualize execution steps, guiding developers to restructure queries for efficiency. Partitioning large tables can improve access speed while reducing locking contention. Regularly updating table statistics ensures the optimizer selects the best execution path. Avoiding correlated subqueries and using batch processing techniques can reduce resource consumption. These approaches together lead to faster, more reliable database systems.\n",
      "Metadata: {'source': 'C:\\\\Users\\\\samantap\\\\AppData\\\\Local\\\\Temp\\\\tmpkx50hwz6\\\\doc_3.txt'}\n",
      "Document 4:\n",
      "In Node.js, the event loop is a key mechanism that allows non-blocking I/O operations on a single thread. Each incoming request is delegated to the system kernel, freeing the JavaScript runtime to handle other events. Async callbacks are queued and executed when the kernel signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its event-driven model, combined with fast V8 execution, supports horizontal scaling and resource-efficient concurrency on modest hardware, making Node.js immensely popular for backend services.\n",
      "Metadata: {'source': 'C:\\\\Users\\\\samantap\\\\AppData\\\\Local\\\\Temp\\\\tmpkx50hwz6\\\\doc_4.txt'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Document loading\n",
    "from langchain_community.document_loaders import DirectoryLoader,TextLoader\n",
    "load = DirectoryLoader(\n",
    "    temp_dir,\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "documents=load.load()\n",
    "for i,doc in enumerate(documents):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e640cdc",
   "metadata": {},
   "source": [
    "### Text Splitting from docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28eaa9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 15 of 5\n",
      "Content: page_content='Python is renowned for its flexible data structures, which include lists, tuples, sets, and dictionaries. Lists are ordered collections that support dynamic resizing and a variety of methods for adding, removing, or searching elements. Tuples are similar to lists but immutable, ensuring that data' metadata={'source': 'C:\\\\Users\\\\samantap\\\\AppData\\\\Local\\\\Temp\\\\tmpkx50hwz6\\\\doc_0.txt'}\n",
      "Metadata: {'source': 'C:\\\\Users\\\\samantap\\\\AppData\\\\Local\\\\Temp\\\\tmpkx50hwz6\\\\doc_0.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Text Splitting\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    separators=[\" \"]\n",
    ")\n",
    "chunks=splitter.split_documents(documents)\n",
    "print(f\"Total chunks: {len(chunks)} of {len(documents)}\")\n",
    "print(f\"Content: {chunks[0]}\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551f490",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3856e937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.043933555483818054,\n",
       " 0.05893440172076225,\n",
       " 0.04817839711904526,\n",
       " 0.07754813879728317,\n",
       " 0.02674437128007412,\n",
       " -0.03762954846024513,\n",
       " -0.0026051148306578398,\n",
       " -0.05994303897023201,\n",
       " -0.0024960089940577745,\n",
       " 0.02207283116877079,\n",
       " 0.048025909811258316,\n",
       " 0.055755313485860825,\n",
       " -0.03894546255469322,\n",
       " -0.026616770774126053,\n",
       " 0.0076933917589485645,\n",
       " -0.026237700134515762,\n",
       " -0.03641606494784355,\n",
       " -0.03781614825129509,\n",
       " 0.07407816499471664,\n",
       " -0.04950505867600441,\n",
       " -0.05852171406149864,\n",
       " -0.06361967325210571,\n",
       " 0.032435014843940735,\n",
       " 0.022008540108799934,\n",
       " -0.07106371223926544,\n",
       " -0.03315779194235802,\n",
       " -0.06941041350364685,\n",
       " -0.05003739148378372,\n",
       " 0.07462679594755173,\n",
       " -0.11113381385803223,\n",
       " -0.01230629812926054,\n",
       " 0.03774565830826759,\n",
       " -0.02803134173154831,\n",
       " 0.014535323716700077,\n",
       " -0.031558554619550705,\n",
       " -0.08058364689350128,\n",
       " 0.05835256725549698,\n",
       " 0.002590067917481065,\n",
       " 0.0392802357673645,\n",
       " 0.025769580155611038,\n",
       " 0.049850545823574066,\n",
       " -0.0017562442226335406,\n",
       " -0.04552978649735451,\n",
       " 0.029260773211717606,\n",
       " -0.10201726108789444,\n",
       " 0.05222873389720917,\n",
       " -0.07908995449542999,\n",
       " -0.010285736061632633,\n",
       " 0.009202471934258938,\n",
       " 0.013073218055069447,\n",
       " -0.04047776013612747,\n",
       " -0.02779252640902996,\n",
       " 0.01246674731373787,\n",
       " 0.06728330999612808,\n",
       " 0.06812482327222824,\n",
       " -0.007571159861981869,\n",
       " -0.006099394988268614,\n",
       " -0.042377717792987823,\n",
       " 0.05178157985210419,\n",
       " -0.015670742839574814,\n",
       " 0.009563641622662544,\n",
       " 0.04123904928565025,\n",
       " 0.021495940163731575,\n",
       " 0.010429336689412594,\n",
       " 0.027334952726960182,\n",
       " 0.01870625466108322,\n",
       " -0.026960738003253937,\n",
       " -0.07005421072244644,\n",
       " -0.10470050573348999,\n",
       " -0.0018987420480698347,\n",
       " 0.017701663076877594,\n",
       " -0.057472579181194305,\n",
       " -0.014422297477722168,\n",
       " 0.00047048315173015,\n",
       " 0.0023323262576013803,\n",
       " -0.025192037224769592,\n",
       " 0.04930044710636139,\n",
       " -0.050961028784513474,\n",
       " 0.06319833546876907,\n",
       " 0.014916478656232357,\n",
       " -0.027076713740825653,\n",
       " -0.04528754949569702,\n",
       " -0.04905946925282478,\n",
       " 0.03749404102563858,\n",
       " 0.03845794126391411,\n",
       " 0.001568990875966847,\n",
       " 0.03099225088953972,\n",
       " 0.020163079723715782,\n",
       " -0.012436363846063614,\n",
       " -0.030672011896967888,\n",
       " -0.0278819240629673,\n",
       " -0.06891823559999466,\n",
       " -0.051367755979299545,\n",
       " 0.021479541435837746,\n",
       " 0.01157472562044859,\n",
       " 0.001254115253686905,\n",
       " 0.018876584246754646,\n",
       " -0.04423188045620918,\n",
       " -0.044981759041547775,\n",
       " -0.003418722189962864,\n",
       " 0.013113114982843399,\n",
       " 0.020009934902191162,\n",
       " 0.12109974771738052,\n",
       " 0.02310747653245926,\n",
       " -0.02201596088707447,\n",
       " -0.03288467600941658,\n",
       " -0.0031551308929920197,\n",
       " 0.00011783662193920463,\n",
       " 0.09914986044168472,\n",
       " 0.016523893922567368,\n",
       " -0.0046967146918177605,\n",
       " -0.014536636881530285,\n",
       " -0.0037108040414750576,\n",
       " 0.09651362150907516,\n",
       " 0.02859078347682953,\n",
       " 0.02134820632636547,\n",
       " -0.0717645063996315,\n",
       " -0.02411423996090889,\n",
       " -0.04409408941864967,\n",
       " -0.10734688490629196,\n",
       " 0.06799457222223282,\n",
       " 0.1304667741060257,\n",
       " -0.0797029510140419,\n",
       " 0.006795077119022608,\n",
       " -0.02375120110809803,\n",
       " -0.046163659542798996,\n",
       " -0.029965076595544815,\n",
       " -3.6941008327124435e-33,\n",
       " 0.07309703528881073,\n",
       " -0.022017158567905426,\n",
       " -0.08614647388458252,\n",
       " -0.07143795490264893,\n",
       " -0.06367422640323639,\n",
       " -0.07218635827302933,\n",
       " -0.005930447019636631,\n",
       " -0.02336413599550724,\n",
       " -0.028365792706608772,\n",
       " 0.04774349555373192,\n",
       " -0.08061760663986206,\n",
       " -0.001564762438647449,\n",
       " 0.013844374567270279,\n",
       " -0.028623618185520172,\n",
       " -0.03353862091898918,\n",
       " -0.11377756297588348,\n",
       " -0.00917635578662157,\n",
       " -0.010810134001076221,\n",
       " 0.032319575548172,\n",
       " 0.05883808061480522,\n",
       " 0.033420857042074203,\n",
       " 0.1079879105091095,\n",
       " -0.03727130591869354,\n",
       " -0.02967708557844162,\n",
       " 0.0517190545797348,\n",
       " -0.022533906623721123,\n",
       " -0.06960906088352203,\n",
       " -0.021447530016303062,\n",
       " -0.02334107831120491,\n",
       " 0.04821997135877609,\n",
       " -0.03587663546204567,\n",
       " -0.04689909145236015,\n",
       " -0.03978736326098442,\n",
       " 0.11081323772668839,\n",
       " -0.01430070586502552,\n",
       " -0.11846451461315155,\n",
       " 0.058291513472795486,\n",
       " -0.06258892267942429,\n",
       " -0.02940407767891884,\n",
       " 0.060323845595121384,\n",
       " -0.0024441315326839685,\n",
       " 0.01601160131394863,\n",
       " 0.026723401620984077,\n",
       " 0.024953022599220276,\n",
       " -0.06493189930915833,\n",
       " -0.010680188424885273,\n",
       " 0.028146496042609215,\n",
       " 0.010356348007917404,\n",
       " -0.0006636060425080359,\n",
       " 0.01981864497065544,\n",
       " -0.03042880818247795,\n",
       " 0.006284233182668686,\n",
       " 0.05152680724859238,\n",
       " -0.04753753915429115,\n",
       " -0.06444213539361954,\n",
       " 0.09550314396619797,\n",
       " 0.07558581233024597,\n",
       " -0.028157468885183334,\n",
       " -0.03499656543135643,\n",
       " 0.10181640088558197,\n",
       " 0.019873222336173058,\n",
       " -0.036803700029850006,\n",
       " 0.002935214899480343,\n",
       " -0.05007451772689819,\n",
       " 0.1509321630001068,\n",
       " -0.061607927083969116,\n",
       " -0.0858813226222992,\n",
       " 0.007139905821532011,\n",
       " -0.01330658234655857,\n",
       " 0.07804052531719208,\n",
       " 0.0175250805914402,\n",
       " 0.04212793707847595,\n",
       " 0.0357939712703228,\n",
       " -0.13295048475265503,\n",
       " 0.03569706901907921,\n",
       " -0.02031167596578598,\n",
       " 0.012490971013903618,\n",
       " -0.0380355566740036,\n",
       " 0.04915430396795273,\n",
       " -0.01565408892929554,\n",
       " 0.12141826003789902,\n",
       " -0.08086445182561874,\n",
       " -0.046878181397914886,\n",
       " 0.04108429700136185,\n",
       " -0.018431775271892548,\n",
       " 0.06696905940771103,\n",
       " 0.004335932899266481,\n",
       " 0.022731535136699677,\n",
       " -0.013642888516187668,\n",
       " -0.045323844999074936,\n",
       " -0.039282944053411484,\n",
       " -0.006298864725977182,\n",
       " 0.05296100676059723,\n",
       " -0.03690645843744278,\n",
       " 0.07116773724555969,\n",
       " 2.3334323261730595e-33,\n",
       " 0.105231374502182,\n",
       " -0.04818745329976082,\n",
       " 0.06959188729524612,\n",
       " 0.06569761782884598,\n",
       " -0.04651489481329918,\n",
       " 0.05144922807812691,\n",
       " -0.012447536922991276,\n",
       " 0.03208721801638603,\n",
       " -0.09233567118644714,\n",
       " 0.050093282014131546,\n",
       " -0.03288758918642998,\n",
       " 0.013913878239691257,\n",
       " -0.0008702629711478949,\n",
       " -0.004909062292426825,\n",
       " 0.10394640266895294,\n",
       " 0.00032157512032426894,\n",
       " 0.052811022847890854,\n",
       " -0.011799043975770473,\n",
       " 0.023156559094786644,\n",
       " 0.013176850974559784,\n",
       " -0.052596259862184525,\n",
       " 0.0326702781021595,\n",
       " 0.00030868290923535824,\n",
       " 0.06411287933588028,\n",
       " 0.03885011374950409,\n",
       " 0.05880085006356239,\n",
       " 0.08297933638095856,\n",
       " -0.018814953044056892,\n",
       " -0.022637730464339256,\n",
       " -0.10047364979982376,\n",
       " -0.038375284522771835,\n",
       " -0.058808136731386185,\n",
       " 0.0018241782672703266,\n",
       " -0.04269954562187195,\n",
       " 0.025019558146595955,\n",
       " 0.06400599330663681,\n",
       " -0.03774828463792801,\n",
       " -0.006839035078883171,\n",
       " -0.0025460815522819757,\n",
       " -0.09760429710149765,\n",
       " 0.018847594037652016,\n",
       " -0.0008832094608806074,\n",
       " 0.017361199483275414,\n",
       " 0.07107910513877869,\n",
       " 0.03303929418325424,\n",
       " 0.006934212055057287,\n",
       " -0.05605234205722809,\n",
       " 0.05146351084113121,\n",
       " -0.04295423626899719,\n",
       " 0.046007778495550156,\n",
       " -0.008788290433585644,\n",
       " 0.031728923320770264,\n",
       " 0.04939655587077141,\n",
       " 0.02951902151107788,\n",
       " -0.05051926150918007,\n",
       " -0.05431870371103287,\n",
       " 0.0001499512291047722,\n",
       " -0.027661476284265518,\n",
       " 0.034687891602516174,\n",
       " -0.02108898013830185,\n",
       " 0.013805982656776905,\n",
       " 0.029988665133714676,\n",
       " 0.013974461704492569,\n",
       " -0.0042647188529372215,\n",
       " -0.015033715404570103,\n",
       " -0.08760954439640045,\n",
       " -0.06850539892911911,\n",
       " -0.042814210057258606,\n",
       " 0.07769452780485153,\n",
       " -0.07102853804826736,\n",
       " -0.007376902736723423,\n",
       " 0.021372701972723007,\n",
       " 0.01355623360723257,\n",
       " -0.07904644310474396,\n",
       " 0.005476667080074549,\n",
       " 0.08306633681058884,\n",
       " 0.11414802819490433,\n",
       " 0.0018076023552566767,\n",
       " 0.08754914999008179,\n",
       " -0.04160451143980026,\n",
       " 0.015541651286184788,\n",
       " -0.010120656341314316,\n",
       " -0.007324368692934513,\n",
       " 0.010796631686389446,\n",
       " -0.06628164649009705,\n",
       " 0.039841361343860626,\n",
       " -0.1167115643620491,\n",
       " 0.06429939717054367,\n",
       " 0.040291961282491684,\n",
       " -0.06547418981790543,\n",
       " 0.01950530707836151,\n",
       " 0.08099953085184097,\n",
       " 0.05364638566970825,\n",
       " 0.07679696381092072,\n",
       " -0.013485214672982693,\n",
       " -1.769190305367374e-08,\n",
       " -0.04439355432987213,\n",
       " 0.009206428192555904,\n",
       " -0.08795901387929916,\n",
       " 0.0426921583712101,\n",
       " 0.07313650846481323,\n",
       " 0.016842715442180634,\n",
       " -0.04032626375555992,\n",
       " 0.018513141199946404,\n",
       " 0.08441726863384247,\n",
       " -0.037447698414325714,\n",
       " 0.030299631878733635,\n",
       " 0.02906418964266777,\n",
       " 0.06368789076805115,\n",
       " 0.02897503972053528,\n",
       " -0.014726998284459114,\n",
       " 0.017754288390278816,\n",
       " -0.033689551055431366,\n",
       " 0.017316123470664024,\n",
       " 0.033787526190280914,\n",
       " 0.17682604491710663,\n",
       " -0.01755334623157978,\n",
       " -0.060307826846838,\n",
       " -0.014339499175548553,\n",
       " -0.02385362610220909,\n",
       " -0.044553130865097046,\n",
       " -0.028985051438212395,\n",
       " -0.0896776095032692,\n",
       " -0.0017593814991414547,\n",
       " -0.026148563250899315,\n",
       " 0.005939967464655638,\n",
       " -0.05183551833033562,\n",
       " 0.08572796732187271,\n",
       " -0.08183988928794861,\n",
       " 0.008354436606168747,\n",
       " 0.04007899761199951,\n",
       " 0.04177643358707428,\n",
       " 0.1045735701918602,\n",
       " -0.002865632064640522,\n",
       " 0.019669104367494583,\n",
       " 0.005810464732348919,\n",
       " 0.013325355015695095,\n",
       " 0.045100126415491104,\n",
       " -0.021758832037448883,\n",
       " -0.013949315994977951,\n",
       " -0.06869927048683167,\n",
       " -0.002941165817901492,\n",
       " -0.031076528131961823,\n",
       " -0.10585445165634155,\n",
       " 0.06916242837905884,\n",
       " -0.04241151735186577,\n",
       " -0.04676824063062668,\n",
       " -0.03647506237030029,\n",
       " 0.04504002630710602,\n",
       " 0.0609816312789917,\n",
       " -0.0656561627984047,\n",
       " -0.00545640429481864,\n",
       " -0.01862267404794693,\n",
       " -0.06314847618341446,\n",
       " -0.03874368965625763,\n",
       " 0.03467337042093277,\n",
       " 0.0555458590388298,\n",
       " 0.05216279253363609,\n",
       " 0.05610649287700653,\n",
       " 0.10206393152475357]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "embeddings=HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    ")\n",
    "embeddings.embed_query(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1181460e",
   "metadata": {},
   "source": [
    "#### Storing into ChromaDB using HuggingFace Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adfbb696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samantap\\AppData\\Local\\Temp\\ipykernel_9920\\3120618224.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  VECTOR_STORE = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created at ./chromaDB\n",
      "Number of vectors: 65\n"
     ]
    }
   ],
   "source": [
    "# Directory of ChromaDB\n",
    "persistant_directory = \"./chromaDB\"\n",
    "\n",
    "VECTOR_STORE = Chroma(\n",
    "    persist_directory=persistant_directory,\n",
    "    embedding_function=HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    ),\n",
    "    collection_name=\"Rag_collection\"\n",
    ")\n",
    "VECTOR_STORE.add_documents(chunks)\n",
    "print(f\"Vector store created at {persistant_directory}\")\n",
    "print(f\"Number of vectors: {VECTOR_STORE._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "774c5738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmppjdesij8\\\\doc_4.txt'}, page_content='signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its'), Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmpb9s9tz0u\\\\doc_4.txt'}, page_content='signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its'), Document(metadata={'source': 'C:\\\\Users\\\\samantap\\\\AppData\\\\Local\\\\Temp\\\\tmpus8vjq_n\\\\doc_4.txt'}, page_content='signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its')]\n",
      "[(Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmppjdesij8\\\\doc_4.txt'}, page_content='signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its'), 0.8995416164398193), (Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmpb9s9tz0u\\\\doc_4.txt'}, page_content='signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its'), 0.8995416164398193), (Document(metadata={'source': 'C:\\\\Users\\\\samantap\\\\AppData\\\\Local\\\\Temp\\\\tmpus8vjq_n\\\\doc_4.txt'}, page_content='signals completion, drastically improving throughput. Promises and async/await syntax further simplify asynchronous code management, reducing callback hell and enhancing maintainability. Node.js excels in microservices, web servers, and real-time applications like chat or streaming services. Its'), 0.8995416164398193)]\n"
     ]
    }
   ],
   "source": [
    "query=\"How to work with Node js?\"\n",
    "similar_chunks=VECTOR_STORE.similarity_search(query, k=3)\n",
    "print(similar_chunks)\n",
    "similar_chunks_with_score=VECTOR_STORE.similarity_search_with_score(query,k=3)\n",
    "print(similar_chunks_with_score)# it prints the similar chunks with their scores or ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ae139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "load_dotenv()\n",
    "OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df191e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### What is Agentic AI?\n",
      "\n",
      "Agentic AI refers to a class of artificial intelligence systems designed to act **autonomously like intelligent agents**. These aren't just passive tools that respond to queries (like a basic chatbot); instead, they **perceive their environment, reason about goals, plan actions, and execute them** to achieve objectives with minimal human intervention. Think of them as digital \"assistants on steroids\" that can handle complex, multi-step tasks in the real world.\n",
      "\n",
      "#### Key Differences from Traditional/Generative AI\n",
      "- **Generative AI** (e.g., ChatGPT, DALL-E): Great at creating text, images, or code *in response to prompts*. It's reactiveâ€”one input, one output.\n",
      "- **Agentic AI**: Proactive and **goal-directed**. It can:\n",
      "  - Break down big goals into sub-tasks.\n",
      "  - Use tools (e.g., web search, APIs, code execution).\n",
      "  - Learn from feedback and iterate.\n",
      "  - Operate in loops: Observe â†’ Plan â†’ Act â†’ Reflect â†’ Repeat.\n",
      "\n",
      "Analogy: If generative AI is a calculator (you punch in numbers, get a result), agentic AI is like a personal project manager who researches, delegates, and adjusts until the job's done.\n",
      "\n",
      "#### Core Components of Agentic AI\n",
      "Most agentic systems are built on large language models (LLMs) like GPT-4 or Grok, enhanced with these building blocks:\n",
      "1. **Perception/Observation**: Gathers data from the environment (e.g., APIs, sensors, user input).\n",
      "2. **Memory**: Short-term (context window) and long-term (vector databases) to remember past actions.\n",
      "3. **Reasoning & Planning**: Uses techniques like:\n",
      "   - Chain-of-Thought (CoT): Step-by-step thinking.\n",
      "   - ReAct (Reason + Act): Alternates reasoning with actions.\n",
      "   - Tree-of-Thoughts: Explores multiple paths.\n",
      "4. **Tools/Actions**: Interfaces with external systems (e.g., email, browsers, calculators).\n",
      "5. **Reflection & Self-Improvement**: Evaluates outcomes and refines strategies.\n",
      "\n",
      "Popular frameworks:\n",
      "- **Auto-GPT, BabyAGI**: Early autonomous agents that loop until goals are met.\n",
      "- **LangChain/LangGraph**: For building customizable agents.\n",
      "- **CrewAI, MultiOn**: Multi-agent systems where specialized \"agents\" collaborate (e.g., one researches, another summarizes).\n",
      "\n",
      "#### Real-World Examples\n",
      "- **Research Agent**: Give it \"Plan a trip to Tokyo under $2000.\" It searches flights, hotels, compares prices, books if authorized.\n",
      "- **Coding Agent**: \"Build a web app for task management.\" It writes code, tests it, deploys to Vercel.\n",
      "- **xAI's Grok**: I have agentic capabilities! I can use tools like web search, code execution, or even simulate multi-step reasoning in responses.\n",
      "- Enterprise: Devin (Cognition Labs) for software engineering; agents in Salesforce or Microsoft Copilot for workflows.\n",
      "\n",
      "#### Challenges & Future\n",
      "- **Hallucinations & Safety**: Agents can go off-rails (e.g., infinite loops, wrong actions). Solutions: Guardrails, human-in-loop.\n",
      "- **Scalability**: Compute-intensive for long tasks.\n",
      "- **Future**: Expect \"agent swarms\" (teams of agents), integration with robotics (e.g., Tesla Optimus), and AGI-level autonomy. By 2025, agentic AI could automate 30-50% of knowledge work (per estimates from McKinsey/Andrew Ng).\n",
      "\n",
      "If you want a demo, more details on building one, or examples in code, just ask! ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# Using free LLM to know the working of Agentic AI\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# First API call with reasoning\n",
    "response = client.chat.completions.create(\n",
    "  model=\"x-ai/grok-4.1-fast\",\n",
    "  messages=[\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you explain me about Agentic AI?\"\n",
    "          }\n",
    "        ],\n",
    "  extra_body={\"reasoning\": {\"enabled\": True}}\n",
    ")\n",
    "response=response.choices[0].message.content\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16f98a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"**Paris** is the capital of France. It's the largest city in the country and has been the political, cultural, and economic center since 987 AD.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 138, 'prompt_tokens': 163, 'total_tokens': 301, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 106, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'x-ai/grok-4.1-fast:free', 'system_fingerprint': None, 'id': 'gen-1764225191-LvadxnABMOd9PMCcEbAl', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--49b3e83f-e7d9-4e9f-ab85-d55c8ef8661e-0' usage_metadata={'input_tokens': 163, 'output_tokens': 138, 'total_tokens': 301, 'input_token_details': {}, 'output_token_details': {'reasoning': 106}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(\n",
    "    model=\"x-ai/grok-4.1-fast\",\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "response=llm.invoke(\"What is the capital of France?\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64220e",
   "metadata": {},
   "source": [
    "# Using RAG Chains with Chormadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02da99a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001A8F4CF3CB0>, search_kwargs={'k': 4})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With out using Langchain Expression Language\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "retriever=VECTOR_STORE.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4},\n",
    ")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71cb1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom prompts\n",
    "system_prompt=\"\"\"\n",
    "Use the following chunks as reference for the answer of the question \n",
    "Provide a answer of nearly 200-300 words and use the chunks to gather the answer also use your own knowledge and experience\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ca452b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nUse the following chunks as reference for the answer of the question \\nProvide a answer of nearly 200-300 words and use the chunks to gather the answer also use your own knowledge and experience\\n\\nContext:\\n{context}\\n\\nAnswer:\\n\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001A88CB81E50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001A88CA7FA10>, root_client=<openai.OpenAI object at 0x000001A88CB81BD0>, root_async_client=<openai.AsyncOpenAI object at 0x000001A88CA7F770>, model_name='x-ai/grok-4.1-fast', model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://openrouter.ai/api/v1')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting in the chain using create_stuff_document_chain-> It will help to combine all the chunks and provide that to the prompt and then that will be passed to the LLM\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381136c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The total chain is something like this:\\n\n",
    "Input â†’ Format Documents â†’ Create Prompt â†’ LLM Call â†’ Parse Output<br>\n",
    "**Now what internally happens is that RunnableBinding wraps the whole chain and in that at first it accumulates\n",
    "the context using format_docs and then it provide that to the ChatPromptTemplate and then it passed it to the ChatOpenAI , the OpenAI provides a response which later showed using StrOutputParser()**\n",
    "\n",
    "- The format_docs() function is responsible for formatting the documents into a single string.\n",
    "```\n",
    "def format_docs(chunks):\n",
    "return \"\\n\\n\".join(chunks.page_content for doc in docs)\n",
    "```\n",
    "\n",
    "- The ChatPromptTemplate() function is responsible for creating the prompt for the LLM.\n",
    "```\n",
    "ChatPromptTemplate(\n",
    "    input_variables=['context', 'question'],\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                template='''Use the following chunks as reference for the answer of the question \n",
    "Provide a answer of nearly 200-300 words and use the chunks to gather the answer also use your own knowledge and experience\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\n",
    "'''\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(template='{question}')\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "- The ChatOpenAI() function is responsible for calling the LLM.\n",
    "```ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=1000)```\n",
    "\n",
    "- The StrOutputParser() function is responsible for parsing the output of the LLM.\n",
    "```StrOutputParser(output_key='text')```\n",
    "\n",
    "- The runnable_binding is responsible for binding the above functions together.\n",
    "```runnable_binding = RunnableBinding(format_docs, ChatPromptTemplate, ChatOpenAI, StrOutputParser)```\n",
    "\n",
    "- The runnable is something like this:\\n\n",
    "```runnable = runnable_binding.bind(context=context, question=question)```\n",
    "\n",
    "- The runnable is something like this:\\n\n",
    "```runnable.run()```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b343af06",
   "metadata": {},
   "source": [
    "#### Final Rag with both Chunks and Chormadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05efadbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001A8F4CF3CB0>, search_kwargs={'k': 4}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nUse the following chunks as reference for the answer of the question \\nProvide a answer of nearly 200-300 words and use the chunks to gather the answer also use your own knowledge and experience\\n\\nContext:\\n{context}\\n\\nAnswer:\\n\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001A88CB81E50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001A88CA7FA10>, root_client=<openai.OpenAI object at 0x000001A88CB81BD0>, root_async_client=<openai.AsyncOpenAI object at 0x000001A88CA7F770>, model_name='x-ai/grok-4.1-fast', model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://openrouter.ai/api/v1')\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain=create_retrieval_chain(retriever,document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1985c0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'I want to know about vector database as a beginner', 'context': [Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmppjdesij8\\\\doc_2.txt'}, page_content='Vector databases have emerged as critical infrastructure for AI-driven applications by enabling the fast, approximate search of high-dimensional embeddings. Unlike traditional relational stores, vector DBs represent data points as mathematical vectors, supporting similarity queries, KNN search, and'), Document(metadata={'source': 'C:\\\\Users\\\\ps19j\\\\AppData\\\\Local\\\\Temp\\\\tmpb9s9tz0u\\\\doc_2.txt'}, page_content='Vector databases have emerged as critical infrastructure for AI-driven applications by enabling the fast, approximate search of high-dimensional embeddings. Unlike traditional relational stores, vector DBs represent data points as mathematical vectors, supporting similarity queries, KNN search, and'), Document(metadata={'source': 'C:\\\\Users\\\\samantap\\\\AppData\\\\Local\\\\Temp\\\\tmpus8vjq_n\\\\doc_2.txt'}, page_content='Vector databases have emerged as critical infrastructure for AI-driven applications by enabling the fast, approximate search of high-dimensional embeddings. Unlike traditional relational stores, vector DBs represent data points as mathematical vectors, supporting similarity queries, KNN search, and'), Document(metadata={'source': 'C:\\\\Users\\\\samantap\\\\AppData\\\\Local\\\\Temp\\\\tmpkx50hwz6\\\\doc_2.txt'}, page_content='Vector databases have emerged as critical infrastructure for AI-driven applications by enabling the fast, approximate search of high-dimensional embeddings. Unlike traditional relational stores, vector DBs represent data points as mathematical vectors, supporting similarity queries, KNN search, and')], 'answer': '### What Are Vector Databases? A Beginner\\'s Guide\\n\\nVector databases have emerged as critical infrastructure for AI-driven applications by enabling the fast, approximate search of high-dimensional embeddings. If you\\'re new to this, think of them as a supercharged way to store and search data that\\'s not just numbers or text, but **mathematical representations** of things like images, text, or audio.\\n\\n#### Traditional Databases vs. Vector Databases\\nTraditional databases (like SQL ones: MySQL, PostgreSQL) excel at exact matchesâ€”e.g., \"find users where age = 25.\" They use tables with rows and columns. But in AI, data is often converted into **vectors** (lists of numbers, like [0.1, 0.5, -0.2, ...]) by models like OpenAI\\'s embeddings API or Sentence Transformers. These vectors capture **meaning** or **similarity**â€”e.g., \"king\" is close to \"queen\" in vector space, but far from \"apple.\"\\n\\nVector DBs (e.g., Pinecone, Weaviate, Milvus, Chroma) store these high-dimensional vectors (often 768+ dimensions) and support **similarity searches** like:\\n- **KNN (K-Nearest Neighbors)**: Find the top 10 most similar items.\\n- Metrics: Cosine similarity, Euclidean distance.\\n\\nThey use smart indexing (like HNSW or IVF) for lightning-fast **approximate nearest neighbor (ANN)** searches on millions of vectors, which exact searches couldn\\'t handle efficiently.\\n\\n#### Why They Matter for AI\\nImagine:\\n- **Semantic search**: Query \"best sci-fi movies\" and get results based on plot embeddings, not keywords.\\n- **Recommendations**: Netflix-like \"similar products.\"\\n- **Image/Video search**: Find visually similar photos.\\n- **RAG (Retrieval-Augmented Generation)**: Power ChatGPT plugins by fetching relevant docs.\\n\\n#### Getting Started\\n1. Generate embeddings (free tools: Hugging Face).\\n2. Pick an open-source DB like Chroma (local, easy) or cloud like Pinecone.\\n3. Code example (Python): `index.query(vector=your_embedding, top_k=5)`.\\n\\nVector DBs bridge AI models and real-world apps, making \"fuzzy\" searches practical. Dive in with free tiersâ€”it\\'s addictive! (248 words)'}\n"
     ]
    }
   ],
   "source": [
    "# Final chain\n",
    "# Retriver will retrive based on query from the db -> retreived dataw will passed as context\n",
    "# -> context will be passed to the LLM\n",
    "# -> LLM will generate a response based on the custom prompt\n",
    "\n",
    "last_response=rag_chain.invoke({\"input\":\"I want to know about vector database as a beginner\"})\n",
    "print(last_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f86d3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### What Are Vector Databases? A Beginner\\'s Guide\\n\\nVector databases have emerged as critical infrastructure for AI-driven applications by enabling the fast, approximate search of high-dimensional embeddings. If you\\'re new to this, think of them as a supercharged way to store and search data that\\'s not just numbers or text, but **mathematical representations** of things like images, text, or audio.\\n\\n#### Traditional Databases vs. Vector Databases\\nTraditional databases (like SQL ones: MySQL, PostgreSQL) excel at exact matchesâ€”e.g., \"find users where age = 25.\" They use tables with rows and columns. But in AI, data is often converted into **vectors** (lists of numbers, like [0.1, 0.5, -0.2, ...]) by models like OpenAI\\'s embeddings API or Sentence Transformers. These vectors capture **meaning** or **similarity**â€”e.g., \"king\" is close to \"queen\" in vector space, but far from \"apple.\"\\n\\nVector DBs (e.g., Pinecone, Weaviate, Milvus, Chroma) store these high-dimensional vectors (often 768+ dimensions) and support **similarity searches** like:\\n- **KNN (K-Nearest Neighbors)**: Find the top 10 most similar items.\\n- Metrics: Cosine similarity, Euclidean distance.\\n\\nThey use smart indexing (like HNSW or IVF) for lightning-fast **approximate nearest neighbor (ANN)** searches on millions of vectors, which exact searches couldn\\'t handle efficiently.\\n\\n#### Why They Matter for AI\\nImagine:\\n- **Semantic search**: Query \"best sci-fi movies\" and get results based on plot embeddings, not keywords.\\n- **Recommendations**: Netflix-like \"similar products.\"\\n- **Image/Video search**: Find visually similar photos.\\n- **RAG (Retrieval-Augmented Generation)**: Power ChatGPT plugins by fetching relevant docs.\\n\\n#### Getting Started\\n1. Generate embeddings (free tools: Hugging Face).\\n2. Pick an open-source DB like Chroma (local, easy) or cloud like Pinecone.\\n3. Code example (Python): `index.query(vector=your_embedding, top_k=5)`.\\n\\nVector DBs bridge AI models and real-world apps, making \"fuzzy\" searches practical. Dive in with free tiersâ€”it\\'s addictive! (248 words)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b31a9",
   "metadata": {},
   "source": [
    "'### What is a Vector Database? A Beginner\\'s Guide\\n\\nImagine you\\'re trying to find a book in a massive library not by its title or author, but by how similar its content feels to what you\\'re thinkingâ€”like \"stories about brave adventurers in magical worlds.\" Traditional databases (like SQL ones) excel at exact matches: \"Give me the book titled *Harry Potter*.\" But vector databases are built for **similarity searches**, treating data as points in a high-dimensional space.\\n\\nAt their core, vector databases store **embeddings**â€”numerical vectors (arrays of numbers) that represent things like text, images, or audio. For example, AI models like BERT or OpenAI\\'s embeddings turn a sentence like \"I love cats\" into a vector [0.2, -0.5, 1.3, ...] capturing its meaning. These vectors live in hundreds or thousands of dimensions, where similar items cluster close together.\\n\\nUnlike relational databases with rows and tables, vector DBs use math like **cosine similarity** or **Euclidean distance** to find the **nearest neighbors (KNN)** quickly. They rely on **approximate nearest neighbors (ANN)** algorithms (e.g., HNSW or IVF) for speed on massive datasetsâ€”think billions of vectorsâ€”without scanning everything.\\n\\n**Why do they matter?** They\\'re the backbone of modern AI apps:\\n- **RAG (Retrieval-Augmented Generation)**: ChatGPT-like systems pull relevant docs for accurate answers.\\n- **Recommendations**: Netflix suggests shows based on your tastes.\\n- **Search & Fraud Detection**: Find similar images or spot anomalies.\\n\\nPopular open-source/free options: **ChromaDB** (super simple for local use), **Weaviate** (GraphQL-friendly with hybrid search), **Pinecone** (managed cloud service). Getting started? Install Chroma via pip, embed text with Hugging Face, and query in Pythonâ€”it\\'s beginner-friendly!\\n\\nVector DBs scale with GPU acceleration, distributed indexes, and metadata filtering for real-world apps. They\\'re exploding in AI because they make \"fuzzy\" searches lightning-fast. Dive in with a tutorial; you\\'ll see why they\\'re essential infrastructure!\\n\\n*(Word count: 278)*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9961145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Future Outlook for TEOCO Corporation\n",
      "\n",
      "TEOCO (The Evans & Company) is a leading provider of big data analytics, AI-driven software, and services primarily for the telecommunications industry. Founded in 1995 and headquartered in Fairfax, Virginia, USA, the company specializes in revenue management, network optimization, fraud detection, and customer experience solutions. It's privately held, serves over 300 telecom operators globally (including giants like Verizon, Vodafone, and AT&T), and has grown through organic expansion and acquisitions (e.g., Asterisk in 2019 for customer analytics).\n",
      "\n",
      "#### Key Strengths Positioning TEOCO for Growth\n",
      "- **Industry Tailwinds**: \n",
      "  - Telecoms are investing heavily in 5G rollout, edge computing, Open RAN, and IoT. TEOCO's platforms like mTOP (network optimization) and RAVE (revenue assurance) are well-suited for these, using AI/ML to reduce costs and improve efficiency.\n",
      "  - Global data explosion and regulatory pressures (e.g., spectrum auctions, sustainability mandates) drive demand for analytics.\n",
      "- **Innovation Focus**: Recent emphasis on AI, generative AI, and automation. For instance, their SPARK platform integrates predictive analytics for real-time decision-making.\n",
      "- **Financial Health**: Reported steady revenue growth (estimated $200-300M annually pre-2023), with strong profitability due to SaaS/recurring models. No public debt issues noted.\n",
      "- **Market Expansion**: Growing presence in Europe, Asia-Pacific, and Latin America; partnerships with hyperscalers like AWS and Google Cloud.\n",
      "\n",
      "#### Predicted Growth Trajectory (2024-2030)\n",
      "| Timeframe | Key Opportunities | Potential Milestones |\n",
      "|-----------|-------------------|----------------------|\n",
      "| **Short-term (2024-2025)** | 5G monetization, AI upgrades for telcos. Double-digit revenue growth (15-20% YoY). | New product launches (e.g., AI fraud prevention); possible M&A for cybersecurity or satellite tech (LEO like Starlink integration). |\n",
      "| **Medium-term (2026-2028)** | 6G R&D, enterprise 5G private networks, sustainability analytics. Enter adjacent sectors like energy/utilities. | Valuation could hit $1B+ (unicorn status); strategic partnerships or partial IPO/spin-off. |\n",
      "| **Long-term (2029+)** | Quantum-safe analytics, full telco cloud migration. Global market share in BSS/OSS could rise to top 5. | Acquisition by a tech giant (e.g., Oracle, IBM) or independent public listing if growth sustains. |\n",
      "\n",
      "**Optimistic Scenario**: 3-5x revenue growth by 2030, driven by telco digital transformation ($1T+ industry spend). TEOCO could become a \"telco AI specialist\" leader.\n",
      "\n",
      "**Pessimistic Risks**:\n",
      "- Intense competition from Amdocs, Ericsson, Subex, and tech giants (Google, AWS entering telco analytics).\n",
      "- Economic slowdowns delaying capex; geopolitical issues affecting global ops.\n",
      "- Talent wars in AI; dependency on a few large clients (top 10 = ~50% revenue?).\n",
      "\n",
      "#### Recent Indicators (as of my last knowledge in 2023)\n",
      "- Strong momentum: Won awards for innovation (e.g., TM Forum Excellence); expanded India ops.\n",
      "- No major red flags; leadership stable under CEO Anil Malhotra.\n",
      "\n",
      "**Disclaimer**: This is speculative analysis based on industry trends and public data up to 2023. The future is uncertainâ€”markets change rapidly. For investment decisions, consult financial advisors, check latest SEC filings (if any), or sources like Crunchbase, PitchBook, or TEOCO's site (teoco.com). Not financial advice.\n",
      "\n",
      "If you have more details (e.g., specific products or regions), I can refine this! ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# Chain using Langchain Expression Language\n",
    "# In this chain no retriver is used directly its just for learning purpose how langchain expression language works\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel,RunnableBinding,RunnablePassthrough\n",
    "llm=ChatOpenAI(\n",
    "    model=\"x-ai/grok-4.1-fast\",\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant with name {name}\"),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "result = chain.invoke({\"name\": \"RAG-Bot\", \"query\": \"Future of Teoco Company?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64e154fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Define docs \n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "custom_prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Use the following chunks as reference for the answer of the question \n",
    "    Provide a answer of nearly 200-300 words and use the chunks to gather the answer also use your own knowledge and experience\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df477359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001A8F4CF3CB0>, search_kwargs={'k': 4})\n",
       "           | RunnableLambda(format_docs),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Use the following chunks as reference for the answer of the question \\n    Provide a answer of nearly 200-300 words and use the chunks to gather the answer also use your own knowledge and experience\\n\\n    Context:\\n    {context}\\n\\n    Answer:\\n    '), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001A88DD6C510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001A88CCCD6D0>, root_client=<openai.OpenAI object at 0x000001A88CA6D940>, root_async_client=<openai.AsyncOpenAI object at 0x000001A88CCCD1D0>, model_name='x-ai/grok-4.1-fast', model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://openrouter.ai/api/v1')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LCEL with RAG\n",
    "rag_chain=(\n",
    "{\"context\":retriever|format_docs,\"question\": RunnablePassthrough() }\n",
    "| custom_prompt\n",
    "| llm\n",
    "| StrOutputParser()\n",
    ")\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "152b05c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Mastering SQL Query Optimization for Scalable Databases\n",
      "\n",
      "SQL query optimization is crucial for scalable database operations, ensuring efficient performance as data volumes grow. One foundational strategy is indexing frequently searched columns, such as those in WHERE clauses, ORDER BY, or JOIN conditions. Indexes act like a book's index, allowing the database engine to quickly locate data without full table scans. However, balance is key: too many indexes can degrade write performance during INSERT, UPDATE, or DELETE operations, as each modification requires updating multiple index structures, increasing I/O overhead and storage needs.\n",
      "\n",
      "To optimize queries effectively, always avoid 'SELECT *'â€”it fetches unnecessary columns, bloating data transfer and memory usage. Instead, explicitly select only required fields (e.g., `SELECT user_id, name FROM users`). Minimize joins to essential tables; excessive multi-table joins can lead to Cartesian products or slow nested loop executions. Prioritize INNER JOINs over OUTER when possible, and ensure join predicates use indexed columns.\n",
      "\n",
      "Craft WHERE clauses that leverage indexed columns for optimal filter efficiency. For instance, `WHERE status = 'active' AND created_at > '2023-01-01'` benefits from composite indexes on (status, created_at). Use equality operators (=) before ranges (> , LIKE with leading wildcards) in composite indexes, following the \"equality-range\" rule.\n",
      "\n",
      "Leverage database-specific tools like MySQL's EXPLAIN or PostgreSQL's EXPLAIN ANALYZE to inspect execution plans. Look for \"Using index\" or low row estimates; avoid \"Using filesort\" or temporary tables. Additional techniques include:\n",
      "\n",
      "- **Limiting results**: Add LIMIT and OFFSET for pagination.\n",
      "- **Subquery optimization**: Rewrite correlated subqueries as JOINs.\n",
      "- **Batch operations**: Group multiple updates/deletes to reduce round-trips.\n",
      "- **Query caching**: Enable for read-heavy workloads (e.g., Redis alongside DB).\n",
      "- **Partitioning/Sharding**: For massive tables, partition by date or range.\n",
      "\n",
      "Regular maintenanceâ€”ANALYZE TABLE, rebuilding indexes, and monitoring slow query logsâ€”sustains performance. In production, tools like New Relic or pgBadger provide insights. By applying these principles, queries can execute in milliseconds instead of seconds, supporting high-traffic applications seamlessly.\n",
      "\n",
      "(Word count: 278)\n"
     ]
    }
   ],
   "source": [
    "response_lcel=rag_chain.invoke(\"How to optimize SQL queries?\")\n",
    "print(response_lcel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e22a8",
   "metadata": {},
   "source": [
    "#### Adding new Document into the Vector Store\n",
    "- Best use case when database is updated or new data came and we want to update the ai assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03f2fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_date=\"\"\"\n",
    "About Reinforcement Learning with Expert Iteration (RLEI)\n",
    "Reinforcement Learning with Expert Iteration (RLEI) is an advanced machine learning technique that combines reinforcement learning with expert knowledge to enhance decision-making processes. In RLEI, an agent learns to make optimal decisions by interacting with an environment, receiving feedback in the form of rewards or penalties. The unique aspect of RLEI is the integration of expert demonstrations, which guide the learning process and help the agent explore more effectively.\n",
    "These demonstrations provide valuable insights into successful strategies, allowing the agent to learn from human expertise while still leveraging the power of reinforcement learning algorithms. RLEI has shown promise in complex domains such as game playing, robotics, and autonomous systems, where expert knowledge can significantly accelerate the learning process and improve overall performance.\n",
    "The technique involves iteratively refining the agent's policy by alternating between learning from expert demonstrations and self-improvement through exploration. This dual approach enables the agent to benefit from both human expertise and autonomous learning, leading to more robust and efficient decision-making capabilities.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c0915c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc=Document(\n",
    "    page_content=sample_date,\n",
    "    metadata={\"source\":\"manual entry\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d37c9ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total new chunks: 5\n"
     ]
    }
   ],
   "source": [
    "spilted_new_docs=splitter.split_documents([new_doc])\n",
    "print(\"Total new chunks:\", len(spilted_new_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f7d6caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New document added to the vector store.\n",
      "Total vectors after addition: 75\n"
     ]
    }
   ],
   "source": [
    "VECTOR_STORE.add_documents(spilted_new_docs)\n",
    "print(\"New document added to the vector store.\")\n",
    "print(\"Total vectors after addition:\", VECTOR_STORE._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16ba9451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**What is Reinforcement Learning with Expert Imitation (RLEI)?**\\n\\nReinforcement Learning with Expert Imitation (RLEI) is an advanced hybrid machine learning paradigm that combines the strengths of imitation learning and traditional reinforcement learning (RL) to accelerate policy optimization in complex environments. By incorporating expert demonstrationsâ€”such as human or high-performing agent trajectoriesâ€”RLEI provides agents with valuable insights into successful strategies from the outset, significantly speeding up the learning process and enhancing overall performance compared to pure RL methods like Q-learning or PPO, which often suffer from sparse rewards and sample inefficiency.\\n\\nThe core technique revolves around an iterative refinement loop. The agent alternates between two phases: (1) **imitation from experts**, where it learns to mimic optimal behaviors via methods like behavioral cloning or generative adversarial imitation learning (GAIL), distilling human expertise into an initial policy; and (2) **self-improvement through exploration**, where RL algorithms (e.g., actor-critic) enable the agent to explore beyond demonstrations, refining the policy via trial-and-error and environmental feedback. This dual approach mitigates the limitations of standalone imitation learning, which can lead to compounding errors (covariate shift), while leveraging RL's ability to discover novel strategies.\\n\\nFrom my experience with RL frameworks like Stable Baselines3 and RLlib, RLEI shines in domains requiring long-horizon planning. For instance, in game playing (e.g., AlphaStar for StarCraft II), robotics (manipulating objects with dexterous hands), and autonomous systems (self-driving cars navigating traffic), expert data bootstraps convergence, reducing training time by orders of magnitude. Benchmarks show 2-10x faster sample efficiency. Challenges include sourcing high-quality experts and handling distribution shifts, often addressed via techniques like DAgger (Dataset Aggregation) for interactive corrections.\\n\\nOverall, RLEI democratizes RL for real-world applications, blending human intuition with algorithmic scalability, paving the way for more robust AI systems.\\n\\n(Word count: 278)\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What you know about Reinforcement Learning with Expert Iteration?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac96d9b8",
   "metadata": {},
   "source": [
    "### Advanced RAG Technique - Using Conversational Memory\n",
    "- create_history_aware_retirver is used to record the conversation of the prevous hisotory it helps to track that so that all conversation occurs in sync\n",
    "- MessagePlaceholder - It is used to placeholder for history in the chat hostory in prompts\n",
    "- HumanMessage - Standard Message for conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dfe5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating the prompt that using the chat history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

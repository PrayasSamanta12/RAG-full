{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00672f6c",
   "metadata": {},
   "source": [
    "### Loading PDF using two libraries PyPDFLoader and PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8798420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG\\Demo Proj\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "# PyMuPDFLoader is required when the pdf have images or complex formatting it is also faster than PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03256f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyPDFLoader to load PDF document...\n",
      "Number of pages loaded: 10\n",
      "sensors\n",
      "Letter\n",
      "Feature Extraction for Dimensionality Reduction in\n",
      "Cellular Networks Performance Analysis\n",
      "Isabel de-la-Bandera 1,*\n",
      " , David Palacios 2, Jessica Mendoza 1\n",
      " and Raquel Barco 1\n",
      "1 Department of Communications Engineering, University of M√°laga, 29071 M√°laga, Spain;\n",
      "jmr@ic.uma.es (J.M.); rbm@ic.uma.es (R.B.)\n",
      "2 Tupl Spain S.L., Tupl Inc., 29010 M√°laga, Spain; david.palacios@tupl.com\n",
      "* Correspondence: ibanderac@ic.uma.es\n",
      "Received: 13 October 2020; Accepted: 1 December 2020; Published: 4 D\n",
      "Metadata: {'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-12-04T20:20:45+08:00', 'author': 'I. de-la-Bandera, D. Palacios, J. Mendoza and R. Barco', 'title': 'Feature Extraction for Dimensionality Reduction in Cellular Networks Performance Analysis', 'subject': \"Next-generation mobile communications networks will have to cope with an extraordinary amount and variety of network performance indicators, causing an increase in the storage needs of the network databases and the degradation of the management functions due to the high-dimensionality of every network observation. In this paper, different techniques for feature extraction are described and proposed as a means for reducing this high dimensionality, to be integrated as an intermediate stage between the monitoring of the network performance indicators and their usage in mobile networks' management functions. Results using a dataset gathered from a live cellular network show the benefits of this approach, in terms both of storage savings and subsequent management function improvements.\", 'keywords': 'dimensionality reduction; feature extraction; mobile networks', 'moddate': '2020-12-04T20:20:45+08:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017/W32TeX) kpathsea version 6.2.3', 'source': 'data/pdffiles/Dimensionality Reduction for Network KPIs.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Using PyPDFLoader to load PDF document...\")\n",
    "loader = PyPDFLoader(\"data/pdffiles/Dimensionality Reduction for Network KPIs.pdf\")\n",
    "documents = loader.load()\n",
    "print(f\"Number of pages loaded: {len(documents)}\")\n",
    "print(documents[0].page_content[:500])  # Print first 500 characters of the first page\n",
    "print(\"Metadata:\", documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e26c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyMuPDFLoader to load PDF document...\n",
      "Number of pages loaded: 10\n",
      "sensors\n",
      "Letter\n",
      "Feature Extraction for Dimensionality Reduction in\n",
      "Cellular Networks Performance Analysis\n",
      "Isabel de-la-Bandera 1,*\n",
      ", David Palacios 2, Jessica Mendoza 1\n",
      "and Raquel Barco 1\n",
      "1\n",
      "Department of Communications Engineering, University of M√°laga, 29071 M√°laga, Spain;\n",
      "jmr@ic.uma.es (J.M.); rbm@ic.uma.es (R.B.)\n",
      "2\n",
      "Tupl Spain S.L., Tupl Inc., 29010 M√°laga, Spain; david.palacios@tupl.com\n",
      "*\n",
      "Correspondence: ibanderac@ic.uma.es\n",
      "Received: 13 October 2020; Accepted: 1 December 2020; Published: 4 Dec\n",
      "Metadata: {'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-12-04T20:20:45+08:00', 'source': 'data/pdffiles/Dimensionality Reduction for Network KPIs.pdf', 'file_path': 'data/pdffiles/Dimensionality Reduction for Network KPIs.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': 'Feature Extraction for Dimensionality Reduction in Cellular Networks Performance Analysis', 'author': 'I. de-la-Bandera, D. Palacios, J. Mendoza and R. Barco', 'subject': \"Next-generation mobile communications networks will have to cope with an extraordinary amount and variety of network performance indicators, causing an increase in the storage needs of the network databases and the degradation of the management functions due to the high-dimensionality of every network observation. In this paper, different techniques for feature extraction are described and proposed as a means for reducing this high dimensionality, to be integrated as an intermediate stage between the monitoring of the network performance indicators and their usage in mobile networks' management functions. Results using a dataset gathered from a live cellular network show the benefits of this approach, in terms both of storage savings and subsequent management function improvements.\", 'keywords': 'dimensionality reduction; feature extraction; mobile networks', 'moddate': '2020-12-04T20:20:45+08:00', 'trapped': '', 'modDate': \"D:20201204202045+08'00'\", 'creationDate': \"D:20201204202045+08'00'\", 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Using PyMuPDFLoader to load PDF document...\")\n",
    "loader = PyMuPDFLoader(\"data/pdffiles/Dimensionality Reduction for Network KPIs.pdf\")\n",
    "documents = loader.load()\n",
    "print(f\"Number of pages loaded: {len(documents)}\")\n",
    "print(documents[0].page_content[:500])  # Print first 500 characters of the first page\n",
    "print(\"Metadata:\", documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "945e9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import fitz\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import uuid\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def deep_clean_text(text: str) -> str:\n",
    " # 1Ô∏è‚É£ Remove control characters (U+0000 - U+001F, except newline and tab)\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b-\\x1f\\x7f-\\x9f]', '', text)\n",
    "\n",
    "    # 2Ô∏è‚É£ Normalize Unicode (e.g. accents, special glyphs)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # 3Ô∏è‚É£ Replace non-breaking spaces with normal spaces\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "\n",
    "    # 4Ô∏è‚É£ Fix linebreak hyphenations: e.g. \"connec-\\ntion\" ‚Üí \"connection\"\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # 5Ô∏è‚É£ Merge broken lines but keep paragraph breaks\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)   # preserve double newlines\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)  # single newline ‚Üí space\n",
    "\n",
    "    # 6Ô∏è‚É£ Collapse multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # 7Ô∏è‚É£ Remove residual LaTeX junk (if any)\n",
    "    text = re.sub(r'\\\\(begin|end)\\{.*?\\}', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\s*', '', text)\n",
    "\n",
    "    # 8Ô∏è‚É£ Trim whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "#Class for loading PDF with image extraction and text cleaning -> Document loading step\n",
    "class SmartPDFLoader:\n",
    "    def __init__(self, \n",
    "                 file_path, \n",
    "                 extract_images=True, \n",
    "                 save_image_dir=\"pdf_images\", \n",
    "                 save_to_json=True, \n",
    "                 output_dir=\"json_doc_output\"):\n",
    "        \"\"\"\n",
    "        Hybrid Smart PDF Loader with:\n",
    "        - Fallback loading (PyMuPDFLoader ‚Üí PyPDFLoader)\n",
    "        - Text cleaning\n",
    "        - Image extraction\n",
    "        - Structure-aware + Recursive chunking\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.extract_images = extract_images\n",
    "        self.save_image_dir = save_image_dir\n",
    "        self.save_to_json = save_to_json\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        os.makedirs(save_image_dir, exist_ok=True)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TEXT CLEANING\n",
    "    # -------------------------------------------------------------------------\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Deep clean PDF text: remove control chars, fix spaces, normalize.\"\"\"\n",
    "        text = deep_clean_text(text)\n",
    "        return text\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # IMAGE EXTRACTION\n",
    "    # -------------------------------------------------------------------------\n",
    "    def extract_images_from_pdf(self) -> list:\n",
    "        \"\"\"Extracts all images with metadata.\"\"\"\n",
    "        images = []\n",
    "        pdf_doc = fitz.open(self.file_path)\n",
    "\n",
    "        for page_idx, page in enumerate(pdf_doc, start=1):\n",
    "            image_list = page.get_images(full=True)\n",
    "            for img_idx, img in enumerate(image_list, start=1):\n",
    "                xref = img[0]\n",
    "                base_image = pdf_doc.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                image_ext = base_image[\"ext\"]\n",
    "                image = Image.open(BytesIO(image_bytes))\n",
    "\n",
    "                image_filename = f\"page_{page_idx}_img_{img_idx}.{image_ext}\"\n",
    "                image_path = os.path.join(self.save_image_dir, image_filename)\n",
    "                image.save(image_path)\n",
    "\n",
    "                images.append({\n",
    "                    \"page\": page_idx,\n",
    "                    \"path\": image_path,\n",
    "                    \"width\": image.width,\n",
    "                    \"height\": image.height,\n",
    "                    \"ext\": image_ext\n",
    "                })\n",
    "        return images\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STRUCTURE-AWARE + RECURSIVE CHUNKER\n",
    "    # -------------------------------------------------------------------------\n",
    "    def structure_aware_chunk(self, docs, max_chars=1200, overlap=150):\n",
    "        \"\"\"Hybrid structure-based and recursive text splitting.\"\"\"\n",
    "        structured_chunks = []\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chars,\n",
    "            chunk_overlap=overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],\n",
    "        )\n",
    "\n",
    "        # Section headers ‚Äî suitable for scientific PDFs\n",
    "        section_pattern = re.compile(\n",
    "            r'(?i)(?:^|\\n)(abstract|introduction|background|related work|methodology|materials|'\n",
    "            r'implementation|approach|results|discussion|conclusion|performance analysis|experiment setup|'\n",
    "            r'results and discussion|references|appendix|\\d+\\.\\s[A-Z].*?)(?=\\n)',\n",
    "            re.MULTILINE\n",
    "        )\n",
    "\n",
    "        for doc in docs:\n",
    "            text = doc.page_content\n",
    "            meta = doc.metadata.copy()\n",
    "\n",
    "            # Split by major sections\n",
    "            sections = []\n",
    "            last_idx = 0\n",
    "            for match in section_pattern.finditer(text):\n",
    "                start = match.start()\n",
    "                if start > last_idx:\n",
    "                    section_text = text[last_idx:start].strip()\n",
    "                    if section_text:\n",
    "                        sections.append(section_text)\n",
    "                last_idx = start\n",
    "            if last_idx < len(text):\n",
    "                sections.append(text[last_idx:].strip())\n",
    "\n",
    "            # Process each section\n",
    "            for section_text in sections:\n",
    "                header_match = re.match(section_pattern, section_text)\n",
    "                section_title = header_match.group(0).strip() if header_match else \"General\"\n",
    "                section_body = section_text[len(section_title):].strip()\n",
    "\n",
    "                sub_doc = [Document(page_content=section_body, metadata={**meta, \"section\": section_title})]\n",
    "                split_docs = splitter.split_documents(sub_doc)\n",
    "\n",
    "                for sd in split_docs:\n",
    "                    sd.metadata[\"chunk_id\"] = str(uuid.uuid4())\n",
    "                    structured_chunks.append(sd)\n",
    "\n",
    "        return structured_chunks\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE TO JSON\n",
    "    # -------------------------------------------------------------------------\n",
    "    def save_to_json_file(self, documents):\n",
    "        json_data = [{\"page_content\": d.page_content, \"metadata\": d.metadata} for d in documents]\n",
    "        json_filename = os.path.splitext(os.path.basename(self.file_path))[0] + \".json\"\n",
    "        json_path = os.path.join(self.output_dir, json_filename)\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"‚úÖ JSON saved: {json_path}\")\n",
    "        return json_path\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # MAIN LOAD METHOD\n",
    "    # -------------------------------------------------------------------------\n",
    "    def load(self):\n",
    "        \"\"\"Load ‚Üí clean ‚Üí structure-aware chunking ‚Üí return list of Document objects.\"\"\"\n",
    "        try:\n",
    "            print(\"üîπ Trying PyMuPDFLoader...\")\n",
    "            loader = PyMuPDFLoader(self.file_path)\n",
    "            documents = loader.load()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è PyMuPDFLoader failed: {e}. Using PyPDFLoader instead.\")\n",
    "            loader = PyPDFLoader(self.file_path)\n",
    "            documents = loader.load()\n",
    "\n",
    "        print(f\"‚úÖ Loaded {len(documents)} raw text chunks.\")\n",
    "\n",
    "        # Clean text\n",
    "        cleaned_docs = []\n",
    "        for d in documents:\n",
    "            clean_content = self.clean_text(d.page_content)\n",
    "            if clean_content:\n",
    "                cleaned_docs.append(Document(page_content=clean_content, metadata=d.metadata))\n",
    "\n",
    "        # Extract images\n",
    "        image_docs = []\n",
    "        if self.extract_images:\n",
    "            imgs = self.extract_images_from_pdf()\n",
    "            for img_data in imgs:\n",
    "                image_docs.append(Document(page_content=\"[IMAGE]\", metadata={\"type\": \"image\", **img_data}))\n",
    "\n",
    "        # Chunk the text docs\n",
    "        chunked_docs = self.structure_aware_chunk(cleaned_docs)\n",
    "        print(f\"‚úÖ Chunked into {len(chunked_docs)} sections.\")\n",
    "\n",
    "        final_docs = chunked_docs + image_docs\n",
    "\n",
    "        # Save cleaned version (optional)\n",
    "        if self.save_to_json:\n",
    "            self.save_to_json_file(chunked_docs)\n",
    "\n",
    "        return final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0306cbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Trying PyMuPDFLoader...\n",
      "‚úÖ Loaded 10 raw text chunks.\n",
      "‚úÖ Chunked into 37 sections.\n",
      "‚úÖ JSON saved: json_doc_output\\Dimensionality Reduction for Network KPIs.json\n",
      "Total documents loaded (text + images): 49\n",
      "Text Document Content (first 100 chars): Letter Feature Extraction for Dimensionality Reduction in Cellular Networks Performance Analysis Isa\n",
      "Metadata: 1124\n",
      "Text Document Content (first 100 chars): . Results using a dataset gathered from a live cellular network show the benefits of this approach, \n",
      "Metadata: 991\n",
      "Text Document Content (first 100 chars): . To know the current network state (e.g., whether the network behavior is sub-optimal or degraded),\n",
      "Metadata: 1089\n",
      "Text Document Content (first 100 chars): . To avoid these problems, an efficient selection of a set of KPIs should be carried out in order to\n",
      "Metadata: 572\n",
      "Text Document Content (first 100 chars): 2020, 20, 6944 2 of 10 that is traditionally selected could not be the most suitable one for some sp\n",
      "Metadata: 1132\n",
      "Text Document Content (first 100 chars): . However, given their supervised nature, the works in [5,6] rely on the availability of a network s\n",
      "Metadata: 1111\n",
      "Text Document Content (first 100 chars): . The application of feature extraction to cellular network management is presented in several works\n",
      "Metadata: 1194\n",
      "Text Document Content (first 100 chars): . In addition, although the work presented in [9] is also based on feature extraction techniques, th\n",
      "Metadata: 1132\n",
      "Text Document Content (first 100 chars): . With this aim, the diagnosis error rate (DER) indicator is used, which stands for the samples‚Äô mis\n",
      "Metadata: 237\n",
      "Text Document Content (first 100 chars): 2020, 20, 6944 3 of 10 The rest of the paper is organized as follows: firstly, the problem formulati\n",
      "Metadata: 1070\n",
      "Text Document Content (first 100 chars): . A feature could be, for example, the number of user connection attempts registered by a base stati\n",
      "Metadata: 1112\n",
      "Text Document Content (first 100 chars): . In the context of performance analysis, diagnosis functions take the shape of classifying systems,\n",
      "Metadata: 992\n",
      "Text Document Content (first 100 chars): 2020, 20, 6944 4 of 10 is lower than the number of the original ones. Feature extraction is a tool t\n",
      "Metadata: 1102\n",
      "Text Document Content (first 100 chars): . In this case, given that the vast majority of feature extraction techniques is unsupervised, only \n",
      "Metadata: 1161\n",
      "Text Document Content (first 100 chars): . The reason why feature extraction techniques get to retain a higher amount of useful information t\n",
      "Metadata: 1114\n",
      "Text Document Content (first 100 chars): . The application methodology followed in this work for all the feature extraction techniques is the\n",
      "Metadata: 477\n",
      "Text Document Content (first 100 chars): 2020, 20, 6944 5 of 10 KPIs since the learning process is different for each method. More details ab\n",
      "Metadata: 1038\n",
      "Text Document Content (first 100 chars): . In the context of mobile networks, given an original set of KPIs of N dimensions, PCA determines t\n",
      "Metadata: 956\n",
      "Text Document Content (first 100 chars): . ‚Ä¢ Independent component analysis (ICA): Unlike PCA, which only looks for the orthogonality of the \n",
      "Metadata: 731\n",
      "Text Document Content (first 100 chars): 2020, 20, 6944 6 of 10 of classification, such as a fault diagnosis based on KPIs, it would be easie\n",
      "Metadata: 1176\n",
      "Text Document Content (first 100 chars): . Figure 4 shows an example of the application of this technique. ‚Ä¢ Spectral embedding, SE: this tec\n",
      "Metadata: 1160\n",
      "Text Document Content (first 100 chars): . Each sample is composed of 286 RAN KPIs and a ground truth label, indicating the network state und\n",
      "Metadata: 756\n",
      "Text Document Content (first 100 chars): 2020, 20, 6944 7 of 10 An LDA (linear discriminant analysis) classifier is used as the diagnosis too\n",
      "Metadata: 1117\n",
      "Text Document Content (first 100 chars): . The same number of synthetic KPIs are considered in all these situations, 10. The dataset has been\n",
      "Metadata: 1129\n",
      "Text Document Content (first 100 chars): . Then, the resulting DERs have been averaged over the 50 repetitions. Finally, a standard normaliza\n",
      "Metadata: 1103\n",
      "Text Document Content (first 100 chars): . In light of this, all the component analysis-based techniques except for kPCA1 provide a lower DER\n",
      "Metadata: 1088\n",
      "Text Document Content (first 100 chars): . It is also possible to find KPIs that are not related to other KPIs. For example, a KPI related to\n",
      "Metadata: 677\n",
      "Text Document Content (first 100 chars): 2020, 20, 6944 8 of 10 Figure 5. Diagnosis error rate (DER) of diagnosis system based on LDA for dif\n",
      "Metadata: 1044\n",
      "Text Document Content (first 100 chars): . Based on this large amount of information, experts must analyze the performance of the network and\n",
      "Metadata: 1157\n",
      "Text Document Content (first 100 chars): . In relation to the former, it is important to take into account that an extremely high reduction m\n",
      "Metadata: 332\n",
      "Text Document Content (first 100 chars): 2020, 20, 6944 9 of 10 considered in the whole fault management framework when it is applied to a ne\n",
      "Metadata: 1073\n",
      "Text Document Content (first 100 chars): . At the expense of losing the meaning of the resulting KPIs, feature extraction techniques allow fo\n",
      "Metadata: 1135\n",
      "Text Document Content (first 100 chars): .B.; project administration, R.B.; funding acquisition, R.B. All authors have read and agreed to the\n",
      "Metadata: 1195\n",
      "Text Document Content (first 100 chars): . 448‚Äì452. 4. Shafiq, M.; Yu, X.; Laghari, A.; Wang, D. Effective feature selection for 5G IM applic\n",
      "Metadata: 1027\n",
      "Text Document Content (first 100 chars): 2020, 20, 6944 10 of 10 9. Mercader, A.; Sue, J.A.; Hasholzner, R.; Brendel, J. Improvements in LTE-\n",
      "Metadata: 1190\n",
      "Text Document Content (first 100 chars): . Jolliffe, I.T. Principal Component Analysis, 2nd ed.; Springer Series in Statistics; Springer: New\n",
      "Metadata: 1071\n",
      "Text Document Content (first 100 chars): . Khatib, E.J.; G√≥mez-Andrades, A.; Serrano, I.; Barco, R. Modelling LTE solved troubleshooting case\n",
      "Metadata: 515\n"
     ]
    }
   ],
   "source": [
    "documents=SmartPDFLoader(\"data/pdffiles/Dimensionality Reduction for Network KPIs.pdf\").load()\n",
    "print(f\"Total chunks loded (text): {len(documents)}\")\n",
    "for chunks in documents:\n",
    "    if chunks.page_content != \"[IMAGE]\":\n",
    "        print(f\"Text Document Content (first 100 chars): {chunks.page_content[:100]}\")\n",
    "        print(f\"Metadata: {len(chunks.page_content)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c99cd",
   "metadata": {},
   "source": [
    "## Custom Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd08b2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
